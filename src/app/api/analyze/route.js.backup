import { NextRequest, NextResponse } from 'next/server';
import { exec } from 'child_process';
import { promisify } from 'util';
import fs from 'fs';
import path from 'path';
import OpenAI from 'openai';
import { isSupabaseAvailable, getUserProfile, updateUserCredits, supabase } from '@/lib/supabase';

const execAsync = promisify(exec);
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
});

// Rate limiting configuration
const RATE_LIMIT_DELAY = 3000; // 3 seconds between requests
const MAX_RETRIES = 2; // Reduced retries to avoid long waits

// Helper function to wait
const wait = (ms) => new Promise(resolve => setTimeout(resolve, ms));

// Helper function to log with timestamp
function logWithTimestamp(message, data = null) {
  const timestamp = new Date().toISOString();
  console.log(`[${timestamp}] ${message}`, data ? JSON.stringify(data, null, 2) : '');
}

// INSERT_START_DEBUG_HELPERS
// ========= ANALYSIS DEBUG HELPERS =========
function logSceneDecision(frameIndex, isNewScene, reasonsArr = []) {
  logWithTimestamp('üßê SceneDecision', {
    frame: frameIndex,
    newScene: isNewScene,
    reasons: reasonsArr.length ? reasonsArr.join('|') : 'continued'
  });
}

function estimateTokens(str = '') {
  return Math.ceil(str.length / 4);
}
// ==========================================
// INSERT_END_DEBUG_HELPERS

// Helper function to update progress
async function updateProgress(requestId, phase, progress, message, details = {}) {
  try {
    await fetch(`${process.env.NEXT_PUBLIC_BASE_URL || 'http://localhost:3000'}/api/progress`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ requestId, phase, progress, message, details })
    });
  } catch (error) {
    // Silently fail - progress tracking is not critical
    console.warn('Failed to update progress:', error.message);
  }
}

// Helper function to handle rate limits
async function handleRateLimit(fn, retries = MAX_RETRIES) {
  const startTime = Date.now();
  try {
    logWithTimestamp(`üîÑ Executing OpenAI API call with ${retries} retries left`);
    const result = await fn();
    const duration = Date.now() - startTime;
    logWithTimestamp(`‚úÖ OpenAI API call successful`, { duration: `${duration}ms` });
    return result;
  } catch (error) {
    const duration = Date.now() - startTime;
    logWithTimestamp(`‚ùå OpenAI API call failed`, { 
      duration: `${duration}ms`,
      error: error.message,
      code: error.code,
      status: error.status,
      headers: error.headers
    });
    
    if ((error.code === 'rate_limit_exceeded' || error.status === 429) && retries > 0) {
      // Extract retry-after from headers if available, otherwise use default delay
      const retryAfterMs = error.headers?.['retry-after-ms'];
      const retryAfterSec = error.headers?.['retry-after'];
      let waitTime = RATE_LIMIT_DELAY;
      
      if (retryAfterMs) {
        waitTime = parseInt(retryAfterMs) + 1000; // Add 1 second buffer
      } else if (retryAfterSec) {
        waitTime = parseInt(retryAfterSec) * 1000 + 1000; // Convert to ms and add buffer
      }
      
      logWithTimestamp(`‚è≥ Rate limit hit, waiting ${waitTime}ms before retry. Retries left: ${retries - 1}`);
      await wait(waitTime);
      return handleRateLimit(fn, retries - 1);
    }
    throw error;
  }
}

async function downloadVideo(url) {
  const startTime = Date.now();
  logWithTimestamp('üé• Starting video download', { url });
  
  try {
    // Validate URL format
    const igUrlPattern = /^https?:\/\/(www\.)?instagram\.com\/(p|reel|tv)\/[\w-]+/;
    if (!igUrlPattern.test(url)) {
      throw new Error(`Invalid Instagram URL format: ${url}`);
    }
    logWithTimestamp('‚úÖ URL validation passed');
    
    // Create a temporary directory for downloads if it doesn't exist
    const downloadDir = path.join(process.cwd(), 'temp');
    logWithTimestamp('üìÅ Checking temp directory', { downloadDir });
    
    if (!fs.existsSync(downloadDir)) {
      fs.mkdirSync(downloadDir);
      logWithTimestamp('üìÅ Created temp directory');
    } else {
      logWithTimestamp('üìÅ Temp directory already exists');
    }

    // Generate a unique filename
    const timestamp = Date.now();
    const outputPath = path.join(downloadDir, `video_${timestamp}.mp4`);
    logWithTimestamp('üìù Generated output path', { outputPath });
    
    // Use yt-dlp to download the video - force merge to single file
    const command = `yt-dlp -f 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best' '${url}' -o '${outputPath}' --merge-output-format mp4 --verbose`;
    logWithTimestamp('üîß Executing yt-dlp command', { command });
    
    const execStartTime = Date.now();
    const { stdout, stderr } = await execAsync(command, { timeout: 120000 }); // 2 minute timeout
    const execDuration = Date.now() - execStartTime;
    
    logWithTimestamp('üìä yt-dlp execution completed', { 
      duration: `${execDuration}ms`,
      stdoutLength: stdout?.length || 0,
      stderrLength: stderr?.length || 0
    });
    
    if (stdout) logWithTimestamp('üìã yt-dlp stdout', { stdout: stdout.substring(0, 1000) + (stdout.length > 1000 ? '...[truncated]' : '') });
    if (stderr) logWithTimestamp('‚ö†Ô∏è yt-dlp stderr', { stderr: stderr.substring(0, 1000) + (stderr.length > 1000 ? '...[truncated]' : '') });

    // Check if file was created - yt-dlp might modify the filename
    let actualVideoPath = outputPath;

    if (!fs.existsSync(outputPath)) {
      logWithTimestamp('‚ö†Ô∏è Expected video file not found, searching for actual downloaded file', { outputPath });
      
      // List all files in the download directory for debugging
      const downloadDir = path.dirname(outputPath);
      const allFiles = fs.readdirSync(downloadDir);
      logWithTimestamp('üìÇ All files in temp directory', { 
        downloadDir,
        allFiles
      });
      
      // Look for any video files that match our timestamp pattern
      const baseFilename = path.basename(outputPath, '.mp4');
      const videoFiles = fs.readdirSync(downloadDir)
        .filter(file => file.startsWith(baseFilename) && (file.endsWith('.mp4') || file.endsWith('.webm')))
        .map(file => path.join(downloadDir, file));
      
      logWithTimestamp('üîç Found potential video files', { 
        videoFiles,
        searchPattern: `${baseFilename}*.(mp4|webm)`
      });
      
      if (videoFiles.length === 0) {
        logWithTimestamp('‚ùå No video files found after download', { 
          outputPath,
          downloadDir,
          baseFilename
        });
        throw new Error('Video download failed - no output files found');
      }
      
      // Use the first (and hopefully only) video file found
      actualVideoPath = videoFiles[0];
      logWithTimestamp('‚úÖ Found actual video file', { 
        expectedPath: outputPath,
        actualPath: actualVideoPath
      });
    }

    // Get file stats
    const stats = fs.statSync(actualVideoPath);
    const duration = Date.now() - startTime;
    logWithTimestamp('‚úÖ Video download successful', { 
      outputPath: actualVideoPath,
      fileSize: `${(stats.size / 1024 / 1024).toFixed(2)} MB`,
      totalDuration: `${duration}ms`
    });
    
    return actualVideoPath;
  } catch (error) {
    const duration = Date.now() - startTime;
    logWithTimestamp('‚ùå Video download failed', { 
      error: error.message,
      duration: `${duration}ms`,
      url
    });
    throw new Error(`Failed to download video: ${error.message}`);
  }
}

async function extractFrames(videoPath) {
  const startTime = Date.now();
  logWithTimestamp('üñºÔ∏è Starting frame extraction', { videoPath });

  try {
    // Validate video file exists
    if (!fs.existsSync(videoPath)) {
      throw new Error(`Video file does not exist: ${videoPath}`);
    }

    const stats = fs.statSync(videoPath);
    logWithTimestamp('üìä Video file stats', { 
      size: `${(stats.size / 1024 / 1024).toFixed(2)} MB`,
      path: videoPath
    });

  const framesDir = path.join(process.cwd(), 'temp', 'frames');
    logWithTimestamp('üìÅ Setting up frames directory', { framesDir });
    
  if (!fs.existsSync(framesDir)) {
    fs.mkdirSync(framesDir, { recursive: true });
      logWithTimestamp('üìÅ Created frames directory');
    } else {
      // Clean existing frames
      const existingFrames = fs.readdirSync(framesDir);
      logWithTimestamp('üßπ Cleaning existing frames', { count: existingFrames.length });
      existingFrames.forEach(file => fs.unlinkSync(path.join(framesDir, file)));
  }

  // Get video duration
    logWithTimestamp('‚è±Ô∏è Getting video duration');
    const durationStartTime = Date.now();
  const { stdout: durationOutput } = await execAsync(
    `ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 "${videoPath}"`
  );
    const durationQueryTime = Date.now() - durationStartTime;
  const duration = parseFloat(durationOutput);

    logWithTimestamp('‚úÖ Video duration obtained', { 
      duration: `${duration}s`,
      queryTime: `${durationQueryTime}ms`
    });

    // Extract two frames every second to catch fast movements
  const outputPattern = path.join(framesDir, 'frame-%d.jpg');
    logWithTimestamp('üéûÔ∏è Extracting frames', { 
      outputPattern,
      expectedFrames: Math.floor(duration * 1), // 1fps = 1 frame per second (50% faster)
      frameRate: '1fps (optimized for speed - analyzing every 1 second)'
    });
    
    const extractStartTime = Date.now();
    const { stdout: extractStdout, stderr: extractStderr } = await execAsync(
      `ffmpeg -i "${videoPath}" -vf fps=1 "${outputPattern}" -y`
    );
    const extractDuration = Date.now() - extractStartTime;
    
    logWithTimestamp('üìä Frame extraction completed', { 
      duration: `${extractDuration}ms`,
      stdout: extractStdout?.substring(0, 500),
      stderr: extractStderr?.substring(0, 500)
    });

  // Get list of extracted frames
    const frameFiles = fs.readdirSync(framesDir)
      .filter(file => file.startsWith('frame-') && file.endsWith('.jpg'));
      
    logWithTimestamp('üìã Frame files found', { 
      count: frameFiles.length,
      files: frameFiles.slice(0, 10) // Log first 10 frames
    });

    const frames = frameFiles
    .map(file => path.join(framesDir, file))
    .sort((a, b) => {
      const aNum = parseInt(a.match(/frame-(\d+)\.jpg/)?.[1] || '0');
      const bNum = parseInt(b.match(/frame-(\d+)\.jpg/)?.[1] || '0');
      return aNum - bNum;
      });

    // Validate frames
    const frameStats = frames.map(frame => {
      const stats = fs.statSync(frame);
      return { path: frame, size: stats.size };
    });

    const totalDuration = Date.now() - startTime;
    logWithTimestamp('‚úÖ Frame extraction successful', { 
      totalFrames: frames.length,
      totalDuration: `${totalDuration}ms`,
      avgFrameSize: `${(frameStats.reduce((sum, f) => sum + f.size, 0) / frameStats.length / 1024).toFixed(2)} KB`
    });

  return frames;
  } catch (error) {
    const duration = Date.now() - startTime;
    logWithTimestamp('‚ùå Frame extraction failed', { 
      error: error.message,
      duration: `${duration}ms`,
      videoPath
    });
    throw new Error(`Failed to extract frames: ${error.message}`);
  }
}

async function extractAudio(videoPath) {
  const startTime = Date.now();
  logWithTimestamp('üîä Starting audio extraction', { videoPath });

  try {
    if (!fs.existsSync(videoPath)) {
      throw new Error(`Video file does not exist: ${videoPath}`);
    }

    const audioPath = path.join(process.cwd(), 'temp', `audio_${Date.now()}.mp3`);
    logWithTimestamp('üéµ Extracting audio', { audioPath });
    
    const extractStartTime = Date.now();
    const { stdout, stderr } = await execAsync(`ffmpeg -i "${videoPath}" -q:a 0 -map a "${audioPath}" -y`);
    const extractDuration = Date.now() - extractStartTime;
    
    if (!fs.existsSync(audioPath)) {
      throw new Error('Audio extraction failed - output file not found');
    }

    const stats = fs.statSync(audioPath);
    const totalDuration = Date.now() - startTime;
    
    logWithTimestamp('‚úÖ Audio extraction successful', { 
      audioPath,
      fileSize: `${(stats.size / 1024).toFixed(2)} KB`,
      extractDuration: `${extractDuration}ms`,
      totalDuration: `${totalDuration}ms`,
      stdout: stdout?.substring(0, 300),
      stderr: stderr?.substring(0, 300)
    });

  return audioPath;
  } catch (error) {
    const duration = Date.now() - startTime;
    logWithTimestamp('‚ùå Audio extraction failed', { 
      error: error.message,
      duration: `${duration}ms`,
      videoPath
    });
    throw new Error(`Failed to extract audio: ${error.message}`);
  }
}

async function analyzeFramesInBatches(frames, requestId = 'unknown') {
  const startTime = Date.now();
  logWithTimestamp('üéØ Starting smart frame batching', { totalFrames: frames.length });

  try {
    // Analyze ALL frames (every 1 second at 1fps) but batch them efficiently
    const allFrames = frames.map((framePath, index) => ({ path: framePath, index }));
    logWithTimestamp('üìä Preparing all frames for analysis', { 
      totalFrames: allFrames.length,
      strategy: 'Every 1 second at 1fps (optimized for speed)'
    });

    // Process in even larger batches with higher API limits - 12 frames per API call for maximum speed
    const batchSize = 12;
    const batches = [];
    for (let i = 0; i < allFrames.length; i += batchSize) {
      batches.push(allFrames.slice(i, i + batchSize));
    }

    logWithTimestamp('üì¶ Created batches', { 
      batchCount: batches.length,
      batchSize,
      avgFramesPerBatch: (allFrames.length / batches.length).toFixed(1)
    });

    const allAnalyses = [];
    
    for (let batchIndex = 0; batchIndex < batches.length; batchIndex++) {
      const batch = batches[batchIndex];
      // Batching accounts for 60% of total progress (5% to 65%)
      const batchProgress = 5 + Math.round((batchIndex / batches.length) * 60);
      const currentTokens = tokenTracker.getCurrentTokenUsage();
      const tokenPercentage = Math.round((currentTokens / tokenTracker.maxTokensPerMinute) * 100);
      await updateProgress(requestId, 'frame_analysis', batchProgress, `Analyzing batch ${batchIndex + 1}/${batches.length} (${batch.length} frames) - Token usage: ${tokenPercentage}%`);
      
      // Add intelligent delay between batches to prevent rate limiting
      if (batchIndex > 0) {
        const currentTokens = tokenTracker.getCurrentTokenUsage();
        const currentRequests = tokenTracker.getCurrentRequestCount();
        
        // If we're approaching limits, add a longer delay (updated for 2M TPM / 5K RPM)
        if (currentTokens > 1500000 || currentRequests > 4000) {
          logWithTimestamp(`‚è≥ High usage detected, adding 2s delay before batch ${batchIndex + 1}`, {
            currentTokens,
            currentRequests
          });
          await new Promise(resolve => setTimeout(resolve, 2000));
        } else if (currentTokens > 1200000 || currentRequests > 3500) {
          logWithTimestamp(`‚è≥ Moderate usage detected, adding 500ms delay before batch ${batchIndex + 1}`, {
            currentTokens,
            currentRequests
          });
          await new Promise(resolve => setTimeout(resolve, 500));
        }
      }
      
      logWithTimestamp(`üîÑ Processing batch ${batchIndex + 1}/${batches.length}`, {
        frameCount: batch.length,
        frameIndices: batch.map(f => f.index)
      });

      try {
        const batchAnalyses = await analyzeBatch(batch, batchIndex);
        allAnalyses.push(...batchAnalyses);
        
        // Minimal delay between batches with higher API limits
        if (batchIndex < batches.length - 1) {
          await wait(200); // 200ms delay between batches (reduced from 500ms for speed)
        }
      } catch (error) {
        logWithTimestamp(`‚ö†Ô∏è Batch ${batchIndex + 1} failed, creating placeholders`, {
          error: error.message,
          frameCount: batch.length
        });
        
        // Create placeholders for failed batch
        const placeholders = batch.map(frame => ({
          frameIndex: frame.index,
          timestamp: `${frame.index}s`,
          analysis: `Batch analysis failed: ${error.message}`
        }));
        allAnalyses.push(...placeholders);
      }
    }

    // All frames analyzed - no interpolation needed
    const duration = Date.now() - startTime;
    logWithTimestamp('‚úÖ Batch analysis complete', {
      totalDuration: `${duration}ms`,
      processedFrames: allAnalyses.length,
      finalFrameCount: allAnalyses.length,
      coverage: '100% frames analyzed (every 1 second at 1fps)'
    });

    return allAnalyses;
  } catch (error) {
    logWithTimestamp('‚ùå Batch analysis failed', { error: error.message });
    throw error;
  }
}

function selectKeyFrames(frames) {
  const keyFrames = [];
  const totalFrames = frames.length;
  
  // Always include first and last frame
  keyFrames.push({ path: frames[0], index: 0 });
  if (totalFrames > 1) {
    keyFrames.push({ path: frames[totalFrames - 1], index: totalFrames - 1 });
  }
  
  // With higher API limits, we can process more frames
  // For videos longer than 15 seconds, sample every 2nd frame
  // For shorter videos, sample every frame
  const interval = totalFrames > 15 ? 2 : 1;
  
  for (let i = interval; i < totalFrames - 1; i += interval) {
    keyFrames.push({ path: frames[i], index: i });
  }
  
  // Sort by index to maintain order
  return keyFrames.sort((a, b) => a.index - b.index);
}

async function analyzeBatch(batch, batchIndex) {
  logWithTimestamp(`üñºÔ∏è Analyzing batch ${batchIndex + 1}`, { frameCount: batch.length });

  try {
    // Read all images in the batch
    const imageData = [];
    for (const frame of batch) {
      if (!fs.existsSync(frame.path)) {
        throw new Error(`Frame file does not exist: ${frame.path}`);
      }
      
      const image = fs.readFileSync(frame.path);
      const base64Image = Buffer.from(image).toString('base64');
      imageData.push({
        index: frame.index,
        base64: base64Image
      });
    }

    logWithTimestamp(`üì§ Sending batch to OpenAI`, { 
      frameCount: batch.length,
      totalImageSize: `${(imageData.reduce((sum, img) => sum + img.base64.length, 0) / 1024).toFixed(0)}KB`
    });

    const result = await handleRateLimit(async () => {
      // Create content array with text prompt and all images
      const content = [
        {
          type: "text",
          text: `Analyze these ${batch.length} video frames with deep contextual understanding. CRITICAL: Focus ONLY on VISUAL storytelling - what's happening on screen. DO NOT analyze background music, lyrics, or audio.

‚ö†Ô∏è CRITICAL INSTRUCTION: You are analyzing IMAGES ONLY. You cannot hear audio. Focus exclusively on what you can SEE in the frames.

VISUAL ANALYSIS PRIORITIES (in order):
1. VISUAL NARRATIVE: What story is being told through images? Actions, expressions, body language
2. ON-SCREEN TEXT: Read and analyze ALL visible text, captions, overlays, graphics, UI elements  
3. VISUAL STORYTELLING: How do cuts, composition, effects support the narrative?
4. CONTEXTUAL MEANING: What is the creator trying to communicate through visuals?
5. EMOTIONAL REACTIONS: Facial expressions, body language, gestures that convey story beats

üö´ IGNORE COMPLETELY: Background music, lyrics, audio, sound effects - you are analyzing SILENT IMAGES

STORY DETECTION FOCUS (Visual Only):
- Character actions and reactions (picking up objects, celebrating, looking confused, pointing, gesturing)
- Location changes (indoor/outdoor, bathroom/car/street, different rooms, settings)
- Visual mood shifts (lighting changes, color grading, effects, filters)
- Dream sequences (surreal elements, impossible situations, reality breaks)
- Emotional progressions (excitement ‚Üí disappointment, confusion ‚Üí realization)
- Physical state changes (standing ‚Üí lying down, clean ‚Üí wet clothes, different poses)
- Object interactions (money, keys, cars, doors, walls, ground)

TEXT ANALYSIS FOCUS:
- Read ALL visible text accurately (captions, overlays, UI text, signs, graphics)
- Analyze HOW text timing relates to visuals (setup/payoff, irony, contrast)
- Identify if text contradicts or supports what's shown visually
- Determine the narrative function of text (exposition, humor, emphasis)

VISUAL TECHNIQUE ANALYSIS:
- Visual cuts and transitions: WHY used? (pacing, reveal, contrast)
- Color/lighting changes: WHY used? (mood change, irony, dream vs reality)
- Text overlays: WHY positioned here? What's the intended impact?
- Facial expressions: What emotion/reaction are they designed to create?
- Composition: How does framing support the message?

Provide analysis for each frame in this exact format:

${batch.map((frame, i) => `FRAME_${frame.index}:
VISUAL_STORY: [describe ONLY what you see happening - actions, expressions, objects, movements] - NARRATIVE_PURPOSE: [why this visual moment matters to the story]
FRAMING: [shot type you can see] - WHY: [reason for this visual choice and its impact]
LIGHTING: [lighting style/mood you can observe] - IMPACT: [how this visual affects viewer emotion]
MOOD: [emotional tone based ONLY on what you see] - PURPOSE: [why this visual emotion is needed here]
ACTION: [what's happening visually - people, objects, movement you can see] - SIGNIFICANCE: [why this visual action matters to the story]
ON_SCREEN_TEXT: [ALL visible text/captions/graphics you can read] - CONTEXT: [what this text reveals about the visual story/message/irony]
VISUAL_EFFECTS: [visual effects you can see - cuts, filters, speed] - INTENT: [why this visual effect was chosen and its purpose]
SETTING: [location/environment you can see] - ROLE: [how this visual setting supports the story]
SUBJECTS: [people/objects you can see in frame] - NARRATIVE_FUNCTION: [their role in the visual story]
CHARACTER_STATE: [physical position, clothing, expressions, gestures you can observe] - STORY_CLUES: [what these visuals reveal about the narrative]
CONTEXTUAL_MEANING: [what this frame communicates about the visual story - based ONLY on what you can see]`).join('\n\n')}

üî• CRITICAL REMINDER: You are analyzing SILENT IMAGES. Focus exclusively on visual elements. Do not reference or analyze any audio, music, lyrics, or sounds. Describe only what you can SEE happening in each frame.`
        }
      ];

      // Add all images to the content
      imageData.forEach(img => {
        content.push({
          type: "image_url",
          image_url: {
            url: `data:image/jpeg;base64,${img.base64}`
          }
        });
      });

      const response = await callOpenAIWithRateLimit([{ role: "user", content }], 3000); // Increased from 2000 with higher limits

      return response.choices[0].message.content;
    });

    // Parse the batch response into individual frame analyses
    const analyses = parseBatchResponse(result, batch);
    
    logWithTimestamp(`‚úÖ Batch analysis successful`, { 
      frameCount: batch.length,
      responseLength: result?.length || 0
    });

    return analyses;
  } catch (error) {
    logWithTimestamp(`‚ùå Batch analysis failed`, { 
      error: error.message,
      batchIndex,
      frameCount: batch.length
    });
    throw error;
  }
}

function parseBatchResponse(response, batch) {
  const analyses = [];
  
  try {
    // Split response by frame markers
    const frameBlocks = response.split(/FRAME_\d+:/);
    
    // Skip the first empty element and process each frame block
    for (let i = 1; i < frameBlocks.length && i <= batch.length; i++) {
      const frameIndex = batch[i - 1].index;
      const analysis = frameBlocks[i].trim();
      
      // Parse the structured analysis to extract contextual meaning
      const contextualMeaning = extractContextualMeaning(analysis);
      
      analyses.push({
        frameIndex,
        timestamp: `${(frameIndex * 0.5).toFixed(1)}s`, // Updated for 2fps
        analysis,
        contextualMeaning
      });
    }
    
    // If parsing failed or we don't have enough analyses, create fallbacks
    while (analyses.length < batch.length) {
      const missingIndex = batch[analyses.length].index;
      analyses.push({
        frameIndex: missingIndex,
        timestamp: `${(missingIndex * 0.5).toFixed(1)}s`,
        analysis: `Analysis parsing incomplete for frame ${missingIndex}`,
        contextualMeaning: 'Context analysis unavailable'
      });
    }
    
  } catch (error) {
    logWithTimestamp('‚ö†Ô∏è Failed to parse batch response, creating fallbacks', { error: error.message });
    
    // Create fallback analyses for all frames in batch
    batch.forEach(frame => {
      analyses.push({
        frameIndex: frame.index,
        timestamp: `${(frame.index * 0.5).toFixed(1)}s`,
        analysis: `Failed to parse batch response: ${error.message}`,
        contextualMeaning: 'Context analysis failed'
      });
    });
  }
  
  return analyses;
}

// Extract contextual meaning from frame analysis
function extractContextualMeaning(analysis) {
  const contextMatch = analysis.match(/CONTEXTUAL_MEANING:\s*(.+?)(?:\n|$)/i);
  if (contextMatch) {
    return contextMatch[1].trim();
  }
  
  // Fallback: look for any line that explains WHY or IMPACT
  const lines = analysis.split('\n');
  const impactLine = lines.find(line => 
    line.includes('WHY:') || 
    line.includes('IMPACT:') || 
    line.includes('PURPOSE:') || 
    line.includes('CONTEXT:')
  );
  
  if (impactLine) {
    const parts = impactLine.split(/(?:WHY:|IMPACT:|PURPOSE:|CONTEXT:)/);
    return parts[1]?.trim() || 'Context analysis available in detailed view';
  }
  
  return 'Context analysis available in detailed view';
}

function interpolateMissingFrames(keyFrameAnalyses, totalFrames) {
  const completeAnalyses = new Array(totalFrames);
  
  // Place key frame analyses in their correct positions
  keyFrameAnalyses.forEach(analysis => {
    completeAnalyses[analysis.frameIndex] = analysis;
  });
  
  // Fill in missing frames with interpolated data
  for (let i = 0; i < totalFrames; i++) {
    if (!completeAnalyses[i]) {
      // Find the nearest analyzed frames
      const prevAnalyzed = findPreviousAnalyzedFrame(completeAnalyses, i);
      const nextAnalyzed = findNextAnalyzedFrame(completeAnalyses, i);
      
      let interpolatedAnalysis;
      if (prevAnalyzed && nextAnalyzed) {
        // Interpolate between two frames
        interpolatedAnalysis = `Similar to frames ${prevAnalyzed.frameIndex}-${nextAnalyzed.frameIndex}: ${prevAnalyzed.analysis.split('\n')[0]}`;
      } else if (prevAnalyzed) {
        // Use previous frame
        interpolatedAnalysis = `Continuation of frame ${prevAnalyzed.frameIndex}: ${prevAnalyzed.analysis.split('\n')[0]}`;
      } else if (nextAnalyzed) {
        // Use next frame
        interpolatedAnalysis = `Leading to frame ${nextAnalyzed.frameIndex}: ${nextAnalyzed.analysis.split('\n')[0]}`;
      } else {
        // Fallback
        interpolatedAnalysis = `Frame ${i}: Analysis not available`;
      }
      
      completeAnalyses[i] = {
        frameIndex: i,
        timestamp: `${i}s`,
        analysis: interpolatedAnalysis
      };
    }
  }
  
  return completeAnalyses;
}

function findPreviousAnalyzedFrame(analyses, index) {
  for (let i = index - 1; i >= 0; i--) {
    if (analyses[i]) return analyses[i];
  }
  return null;
}

function findNextAnalyzedFrame(analyses, index) {
  for (let i = index + 1; i < analyses.length; i++) {
    if (analyses[i]) return analyses[i];
  }
  return null;
}

async function analyzeFrame(framePath, frameIndex) {
  const startTime = Date.now();
  logWithTimestamp(`üñºÔ∏è Analyzing frame ${frameIndex + 1}`, { framePath });

  try {
    if (!fs.existsSync(framePath)) {
      throw new Error(`Frame file does not exist: ${framePath}`);
    }

    const stats = fs.statSync(framePath);
    logWithTimestamp(`üìä Frame ${frameIndex + 1} stats`, { 
      size: `${(stats.size / 1024).toFixed(2)} KB`
    });

  const image = fs.readFileSync(framePath);
  const base64Image = Buffer.from(image).toString('base64');

    logWithTimestamp(`üîç Sending frame ${frameIndex + 1} to OpenAI Vision`, { 
      base64Length: base64Image.length 
    });

    const result = await handleRateLimit(async () => {
    const response = await callOpenAIWithRateLimit([
        {
          role: "user",
          content: [
            {
              type: "text",
                text: `Analyze frame ${frameIndex + 1} concisely:

FRAMING: [shot type]
LIGHTING: [style/mood]
MOOD: [emotional tone]
ACTION: [what's happening]
DIALOGUE: [visible text/overlays]
VISUAL_EFFECTS: [effects/filters]
SETTING: [location/environment]
SUBJECTS: [main focus]

Keep each response brief and specific.`
            },
            {
              type: "image_url",
              image_url: {
                url: `data:image/jpeg;base64,${base64Image}`
              }
            }
          ]
        }
      ], 300);

    return response.choices[0].message.content;
  });

    const totalDuration = Date.now() - startTime;
    logWithTimestamp(`‚úÖ Frame ${frameIndex + 1} analysis complete`, { 
      duration: `${totalDuration}ms`,
      resultLength: result?.length || 0
    });

    return {
      frameIndex,
      timestamp: `${frameIndex}s`,
      analysis: result
    };
  } catch (error) {
    const duration = Date.now() - startTime;
    logWithTimestamp(`‚ùå Frame ${frameIndex + 1} analysis failed`, { 
      error: error.message,
      duration: `${duration}ms`,
      framePath
    });
    throw new Error(`Failed to analyze frame ${frameIndex + 1}: ${error.message}`);
  }
}

async function analyzeAudio(audioPath) {
  const startTime = Date.now();
  logWithTimestamp('üéµ Starting audio analysis', { audioPath });

  try {
    if (!fs.existsSync(audioPath)) {
      throw new Error(`Audio file does not exist: ${audioPath}`);
    }

    const stats = fs.statSync(audioPath);
    logWithTimestamp('üìä Audio file stats', { 
      size: `${(stats.size / 1024).toFixed(2)} KB`
    });

  const audioFile = fs.createReadStream(audioPath);
  
  // First, transcribe the audio
    logWithTimestamp('üó£Ô∏è Starting audio transcription');
    const transcriptionStartTime = Date.now();
    
  const transcription = await handleRateLimit(async () => {
    return await openai.audio.transcriptions.create({
      file: audioFile,
      model: "whisper-1",
      response_format: "verbose_json",
      timestamp_granularities: ["segment"]
    });
  });

    const transcriptionDuration = Date.now() - transcriptionStartTime;
    logWithTimestamp('‚úÖ Audio transcription complete', { 
      duration: `${transcriptionDuration}ms`,
      textLength: transcription.text?.length || 0,
      segmentCount: transcription.segments?.length || 0
    });

  // Then, analyze and separate dialogue from music/lyrics
    logWithTimestamp('üéº Starting audio separation and analysis');
    const analysisStartTime = Date.now();
    
  const separationAnalysis = await handleRateLimit(async () => {
    const response = await callOpenAIWithRateLimit([
        {
          role: "user",
          content: `Analyze this audio transcript and separate DIALOGUE from MUSIC LYRICS. This is CRITICAL for proper video analysis.

FULL TRANSCRIPT: "${transcription.text}"

TIMESTAMPED SEGMENTS:
${transcription.segments?.map(seg => `${seg.start.toFixed(1)}s-${seg.end.toFixed(1)}s: "${seg.text}"`).join('\n') || 'No segments available'}

CRITICAL DISTINCTION RULES:
üéµ MUSIC LYRICS indicators:
- Repetitive phrases or choruses
- Rhyming patterns
- Poetic/metaphorical language
- References to "raindrops", "falling", "sun", "sleeping on the job"
- Melodic/rhythmic speech patterns
- Background vocals or singing
- Song-like structure with verses/chorus

üó£Ô∏è DIALOGUE indicators:
- Direct speech to camera or other people
- Conversational tone
- Instructions or explanations
- Reactions or commentary
- Natural speech patterns
- Narration about what's happening

EXAMPLE ANALYSIS:
- "The raindrops are falling on my head" = MUSIC LYRICS (poetic, metaphorical)
- "So I'm going to show you how to..." = DIALOGUE (instructional)
- "Look at this amazing thing" = DIALOGUE (conversational)

Provide analysis in this JSON format:
{
  "dialogue": {
    "content": "[ONLY spoken dialogue/narration/speech - NOT song lyrics]",
    "segments": [{"start": 0, "end": 5, "text": "dialogue text", "contextualMeaning": "what this reveals about the video"}],
    "primaryContext": "[main message/story revealed through spoken dialogue - ignore lyrics]"
  },
  "musicLyrics": {
    "content": "[ANY sung lyrics, background vocals, or musical content]",
    "mood": "[musical genre/mood]",
    "role": "[how music supports the narrative - usually background/atmospheric]"
  },
  "soundDesign": {
    "effects": "[sound effects, ambient sounds, UI sounds]",
    "purpose": "[how sound design enhances the experience]"
  },
  "contextPriority": "[dialogue/music/effects - which provides most STORY context - usually dialogue]",
  "audioSummary": "[brief summary focusing on DIALOGUE content, noting if music is just background]"
}

CRITICAL: If the transcript contains poetic/lyrical content like "raindrops falling" or similar metaphorical language, classify it as MUSIC LYRICS, not dialogue. Only classify as dialogue if it's clearly someone speaking conversationally or providing narration about the visual content.`
        }
      ], 1200);
    return response.choices[0].message.content;
  });

  // Parse the separation analysis
  let separatedAudio = null;
  try {
    separatedAudio = JSON.parse(separationAnalysis);
    logWithTimestamp('‚úÖ Audio separation successful', {
      hasDialogue: !!separatedAudio.dialogue?.content,
      hasMusicLyrics: !!separatedAudio.musicLyrics?.content,
      contextPriority: separatedAudio.contextPriority
    });
  } catch (parseError) {
    logWithTimestamp('‚ö†Ô∏è Failed to parse audio separation, using fallback');
    separatedAudio = {
      dialogue: { 
        content: transcription.text, 
        primaryContext: 'Audio separation failed - full transcript available' 
      },
      musicLyrics: { content: '', mood: 'Unknown' },
      soundDesign: { effects: 'Unknown', purpose: 'Unknown' },
      contextPriority: 'dialogue',
      audioSummary: 'Full transcript: ' + transcription.text
    };
  }

  // Generate comprehensive audio analysis
  const analysis = `AUDIO CONTEXT ANALYSIS:

PRIMARY DIALOGUE: ${separatedAudio.dialogue?.content || 'None detected'}
Context Priority: ${separatedAudio.contextPriority || 'Unknown'}
Key Message: ${separatedAudio.dialogue?.primaryContext || 'Not available'}

MUSIC/LYRICS: ${separatedAudio.musicLyrics?.content || 'None detected'}
Musical Mood: ${separatedAudio.musicLyrics?.mood || 'Not detected'}

SOUND DESIGN: ${separatedAudio.soundDesign?.effects || 'Standard audio'}

OVERALL CONTEXT: ${separatedAudio.audioSummary || 'Audio provides context through transcript'}`;

    const analysisDuration = Date.now() - analysisStartTime;
    const totalDuration = Date.now() - startTime;
    
    logWithTimestamp('‚úÖ Audio analysis complete', { 
      transcriptionDuration: `${transcriptionDuration}ms`,
      analysisDuration: `${analysisDuration}ms`,
      totalDuration: `${totalDuration}ms`,
      analysisLength: analysis?.length || 0
  });

  return {
    transcription,
    separatedAudio,
    analysis
  };
  } catch (error) {
    const duration = Date.now() - startTime;
    logWithTimestamp('‚ùå Audio analysis failed', { 
      error: error.message,
      duration: `${duration}ms`,
      audioPath
    });
    throw new Error(`Failed to analyze audio: ${error.message}`);
  }
}

async function generateComprehensiveAnalysis(frameAnalyses, audioAnalysis) {
  const startTime = Date.now();
  logWithTimestamp('üöÄ Starting comprehensive analysis (single request optimization)');
  
  try {
    // Add timeout to prevent hanging - increased for complex analysis with higher API limits
    const timeoutPromise = new Promise((_, reject) => 
      setTimeout(() => reject(new Error('Comprehensive analysis timeout after 120 seconds')), 120000)
    );
    const videoLength = (frameAnalyses.length / 2).toFixed(1);
    const transcript = audioAnalysis.transcription?.text || 'No audio transcript';
    
    // Extract all on-screen text from frames
    const allFrameText = frameAnalyses.map(frame => {
      const textMatch = frame.analysis.match(/ON_SCREEN_TEXT:\s*([^-\n]+)/i);
      return textMatch ? textMatch[1].trim() : '';
    }).filter(text => text && text !== 'None detected').join(' | ');

    // Get dialogue vs music context
    const dialogueContent = audioAnalysis.separatedAudio?.dialogue?.content || '';
    const musicContent = audioAnalysis.separatedAudio?.musicLyrics?.content || '';
    const contextPriority = audioAnalysis.separatedAudio?.contextPriority || 'unknown';

    const comprehensivePrompt = `You are a premium video content strategist and analysis expert. Analyze this ${videoLength}-second video and provide a comprehensive, detailed analysis matching professional standards.

VIDEO DATA:
Duration: ${videoLength} seconds
Frame Count: ${frameAnalyses.length}
Audio Transcript: "${transcript}"
On-Screen Text: "${allFrameText}"

DETAILED FRAME ANALYSIS:
${frameAnalyses.slice(0, 12).map((frame, i) => 
  `${(i * 0.5).toFixed(1)}s: ${frame.contextualMeaning || frame.analysis.substring(0, 300)}`
).join('\n')}

AUDIO CONTEXT:
${audioAnalysis.separatedAudio ? `
- Dialogue: ${dialogueContent || 'No dialogue detected - analyzing visual story only'}
- Music/Lyrics: ${musicContent || 'Background music present'}
- Sound Design: ${audioAnalysis.separatedAudio.soundDesign?.effects || 'Standard'}
- Context Priority: ${contextPriority}
- Audio Summary: ${audioAnalysis.separatedAudio.audioSummary || 'Not available'}

‚ö†Ô∏è ANALYSIS FOCUS: ${dialogueContent ? 'Dialogue provides story context' : 'NO DIALOGUE DETECTED - Focus entirely on VISUAL storytelling, character actions, expressions, and on-screen elements'}
` : `Full Audio: ${transcript}`}

CRITICAL SCENE ANALYSIS REQUIREMENTS:
A SCENE is defined by significant changes in:
- VISUAL CUTS: Camera angle changes, shot type changes, framing shifts
- CONTEXT CHANGES: Color grading shifts (color to black & white), lighting changes
- AUDIO TRANSITIONS: Music changes, volume shifts, silence moments, new audio elements
- TEXT REVEALS: New text overlays, text content changes, punchline reveals
- NARRATIVE BEATS: Setup moments, tension building, ironic reveals, punchlines
- VISUAL EFFECTS: Filters applied/removed, speed changes, transitions
- STORY PROGRESSION: Dream/reality shifts, emotional state changes, action sequences
- CHARACTER ACTIONS: Physical state changes, different activities, facial expressions

REQUIREMENTS:
1. MUST identify each distinct scene based on these cinematic changes
2. Each scene should represent a unique narrative beat or visual story moment
3. Look for VISUAL STORYTELLING patterns like setup ‚Üí buildup ‚Üí payoff ‚Üí reveal
4. Identify IRONIC CONTRASTS (dream vs reality, expectation vs outcome)
5. Focus on CHARACTER JOURNEY through visual cues (body language, actions, expressions)
6. Recognize COMEDIC TIMING through visual cuts and reveals

SPECIAL DETECTION PATTERNS:
- DREAM SEQUENCES: Look for surreal elements, impossible situations, reality breaks
- IRONIC REVEALS: Moments where visuals contradict expectations 
- EMOTIONAL REVERSALS: Joy turning to disappointment, confusion to realization
- PHYSICAL STATE CHANGES: Clean to dirty, dry to wet, standing to lying
- LOCATION TRANSITIONS: Indoor/outdoor shifts, different settings
- SETUP/PAYOFF STRUCTURES: Early moments that get paid off later

üéµ BACKGROUND MUSIC HANDLING:
If audio contains only background music/lyrics (no dialogue), focus ENTIRELY on:
- Visual character actions and expressions
- On-screen text and graphics
- Physical comedy and visual gags
- Environmental storytelling
- Color and lighting changes
- Object interactions and discoveries
- Facial expressions and body language
- Visual punchlines and reveals

Return comprehensive analysis in this JSON format:

{
  "scenes": [
    {
      "sceneNumber": 1,
      "timeRange": "0.0s to Xs",
      "duration": "Xs",
      "title": "[Descriptive title focusing on visual action/beat]",
      "description": "[What happens visually in this scene - focus on story progression]",
      "framing": {
        "shotTypes": ["[primary shot types used - e.g., close-up, medium shot, wide shot]"],
        "cameraMovement": "[static/pan/zoom/handheld/tracking/etc]",
        "composition": "[visual composition notes - rule of thirds, centered, etc]"
      },
      "lighting": {
        "style": "[natural/artificial/dramatic/mixed]",
        "mood": "[bright/dark/moody/soft/harsh]",
        "direction": "[front-lit/back-lit/side-lit/top-lit]",
        "quality": "[hard/soft/diffused/natural]"
      },
      "mood": {
        "emotional": "[happy/sad/tense/excited/confused/disappointed/etc]",
        "atmosphere": "[calm/energetic/mysterious/playful/dramatic]",
        "tone": "[serious/playful/dramatic/ironic/hopeful]"
      },
      "actionMovement": {
        "movement": "[description of physical actions and movements]",
        "direction": "[screen direction, eye lines, movement patterns]",
        "pace": "[slow/medium/fast/dynamic/static]"
      },
      "audio": {
        "music": "[music description or 'Background music' or 'None']",
        "soundDesign": "[sound effects, ambient sounds, or 'Standard']",
        "dialogue": "[spoken content or transcribed audio or 'None']"
      },
      "visualEffects": {
        "transitions": "[cuts/fades/wipes/dissolves/jump cuts]",
        "effects": "[filters, color grading, speed changes, overlays]",
        "graphics": "[text overlays, graphics, UI elements, typography]"
      },
      "settingEnvironment": {
        "location": "[specific location description - indoor/outdoor/bathroom/car/etc]",
        "environment": "[indoor/outdoor/studio/natural/urban/etc]",
        "background": "[detailed background elements and their significance]"
      },
      "subjectsFocus": {
        "main": "[primary subjects, people, objects of focus]",
        "secondary": "[supporting elements, background subjects]",
        "focus": "[what specifically draws viewer attention and why]"
      },
      "intentImpactAnalysis": {
        "creatorIntent": "[what the creator is trying to achieve in this specific scene]",
        "howExecuted": "[specific techniques and methods used to achieve the intent]",
        "viewerImpact": "[expected psychological and emotional effect on viewers]",
        "narrativeSignificance": "[how this scene contributes to the overall story/message]"
      },
      "textDialogue": {
        "content": "[exact text content visible on screen or spoken dialogue]",
        "style": "[font style, overlay treatment, visual presentation]"
      }
    }
  ],
  "hooks": [
    {
      "timestamp": "Xs",
      "type": "visual_hook|ironic_reveal|emotional_shift|action_beat",
      "description": "[What makes this moment engaging]",
      "impact": "high|medium|low"
    }
  ],
  "videoCategory": {
    "category": "[primary category based on visual content]",
    "confidence": 95.0,
    "reasoning": "[Why this category based on visual elements]"
  },
  "contextualAnalysis": {
    "creatorIntent": {
      "primaryIntent": "[What the creator wants to achieve through visuals]",
      "howAchieved": "[Specific visual techniques used]",
      "effectivenessFactors": ["visual factor 1", "visual factor 2", "visual factor 3"]
    },
    "humorMechanics": {
      "isHumorous": true,
      "humorType": "[timing/irony/contrast/exaggeration/relatability/visual_gag]",
      "specificTechniques": ["technique1", "technique2"],
      "setupAndPayoff": "[How expectations are set and subverted]",
      "timingElements": "[How timing creates the effect]"
    },
    "narrativeStructure": {
      "setup": "[Initial situation or premise]",
      "conflict": "[Problem or tension introduced]",
      "resolution": "[How it's resolved or punchline delivered]",
      "storytellingDevices": ["device1", "device2"]
    },
    "visualTechniques": {
      "textElements": "[How on-screen text contributes]",
      "visualEffects": "[Freeze frames, transitions, cuts]",
      "facialExpressions": "[How expressions convey meaning]",
      "visualContrasts": "[Before/after, comparison shots]"
    },
    "characters": [
      {
        "description": "[Character description]",
        "represents": "[What they symbolize]",
        "narrative_function": "[Their role in achieving the intent]",
        "key_moments": "[Important actions or reactions]"
      }
    ],
    "messageDelivery": {
      "coreMessage": "[The main point being made]",
      "deliveryMethod": "[How the message is communicated]",
      "memorabilityFactors": ["factor1", "factor2"],
      "audienceAssumptions": ["assumption1", "assumption2"]
    },
    "contextType": "[humor/comparison/tutorial/story/commentary/parody]",
    "targetAudience": "[Who this is made for]",
    "keyInsights": [
      "[Specific insight about how intent is achieved]",
      "[Technical insight about the methods used]",
      "[Strategic insight about why it works]"
    ]
  },
  "strategicOverview": "## üìä Strategic Overview\n\n---\n### üó∫Ô∏è **Story Synopsis** (Overview)\n> Provide a 3-4 sentence, start-to-finish narration of the entire video storyline ‚Äî succinct yet complete, capturing the hook, major beats, and resolution.\n\n---\n### üé¨ **Narrative Arc ‚Äì [genre/format]**\nSetup ‚Üí Build ‚Üí Escalation ‚Üí Payoff/Punchline ‚Üí Resolution\n**Key Beats:** [Setup, Build, Escalation, Payoff, Resolution]\n\n---\n### üí° **Why It Works**\n‚Ä¢ Bullet 1 ‚Äì psychological/emotional trigger\n‚Ä¢ Bullet 2 ‚Äì visual/narrative device\n‚Ä¢ Bullet 3 ‚Äì social/relatability factor\n\n---\n### üîß **Success Formula**\nScene 1 (setup/hook) ‚Üí Scene 2 (development) ‚Üí Scene 3 (escalation) ‚Üí Scene 4 (payoff/punchline). Highlight timing & structural patterns.\n\n---\n### üåç **Universal Principles**\n- Contrast\n- Expectation management\n- Relatability\n- Social proof\n\n---\n### üõ†Ô∏è **Technical Requirements**\n| Essential | Optional |\n|-----------|----------|\n| Clear text overlays | Professional lighting |\n| Readable framing | Advanced sound design |\n| Well-timed cuts | Motion graphics |\n\n---\n### üß≠ **Implementation Framework**\n1. **Pre-Production:** concept, scripting, storyboard\n2. **Production:** filming techniques, talent direction\n3. **Post-Production:** editing, text overlays, audio mixing\n4. **Success Metrics:** engagement rate, watch time, shares\n\n---\n### üöÄ **Viral Potential**\nRate the shareability and scalability of this format (High/Medium/Low) with a one-sentence rationale."

    const apiCallPromise = handleRateLimit(async () => {
      return await callOpenAIWithRateLimit([
          {
            role: "system",
            content: "You are a viral content strategist and video analysis expert. Your role is to analyze technical video breakdowns and transform them into actionable content creation strategies. Focus on: 1) Core psychological and structural elements that make content successful 2) Universal patterns adaptable across contexts 3) Step-by-step replication frameworks 4) WHY content works, not just WHAT happens 5) Adaptable templates for different industries. Prioritize underlying psychology, narrative structure, and replicable techniques over surface-level technical details. Always provide valid JSON with no additional text."
          },
          {
            role: "user",
            content: comprehensivePrompt
          }
        ], 12000); // Increased from 8000 with higher API limits
    });

    const response = await Promise.race([apiCallPromise, timeoutPromise]);

    let result;
    try {
      result = JSON.parse(response.choices[0].message.content);
      logWithTimestamp('‚úÖ JSON parsing successful');
    } catch (parseError) {
      logWithTimestamp('‚ö†Ô∏è JSON parsing failed, attempting to extract JSON', { error: parseError.message });
      
      // Try to extract JSON from response
      const jsonMatch = response.choices[0].message.content.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        result = JSON.parse(jsonMatch[0]);
        logWithTimestamp('‚úÖ JSON extraction successful');
      } else {
        throw new Error('Failed to parse comprehensive analysis JSON');
      }
    }
    
    // Ensure all scenes have the required structure
    if (result.scenes && Array.isArray(result.scenes)) {
      result.scenes = result.scenes.map((scene, index) => {
        const avgSceneDuration = (frameAnalyses.length / result.scenes.length).toFixed(1);
        const sceneStart = (index * frameAnalyses.length / result.scenes.length).toFixed(1);
        const sceneEnd = ((index + 1) * frameAnalyses.length / result.scenes.length).toFixed(1);
        
        return {
          sceneNumber: scene.sceneNumber || index + 1,
          duration: scene.duration || `${avgSceneDuration}s`,
          timeRange: scene.timeRange || `${sceneStart}s - ${sceneEnd}s`,
          title: scene.title || `Scene ${index + 1}`,
          description: scene.description || "Scene analysis in progress",
          framing: {
            shotTypes: scene.framing?.shotTypes || ["Medium shot"],
            cameraMovement: scene.framing?.cameraMovement || "Static",
            composition: scene.framing?.composition || "Standard"
          },
          lighting: {
            style: scene.lighting?.style || "Natural",
            mood: scene.lighting?.mood || "Neutral",
            direction: scene.lighting?.direction || "Front-lit",
            quality: scene.lighting?.quality || "Soft"
          },
          mood: {
            emotional: scene.mood?.emotional || "Neutral",
            atmosphere: scene.mood?.atmosphere || "Calm",
            tone: scene.mood?.tone || "Casual"
          },
          actionMovement: {
            movement: scene.actionMovement?.movement || "Various actions",
            direction: scene.actionMovement?.direction || "Mixed",
            pace: scene.actionMovement?.pace || "Medium"
          },
          audio: {
            music: scene.audio?.music || "Background music",
            soundDesign: scene.audio?.soundDesign || "Standard",
            dialogue: scene.audio?.dialogue || audioAnalysis.transcription?.text || "None"
          },
          visualEffects: {
            transitions: scene.visualEffects?.transitions || "Standard cuts",
            effects: scene.visualEffects?.effects || "Basic",
            graphics: scene.visualEffects?.graphics || "Text overlays"
          },
          settingEnvironment: {
            location: scene.settingEnvironment?.location || "Various",
            environment: scene.settingEnvironment?.environment || "Mixed",
            background: scene.settingEnvironment?.background || "Standard"
          },
          subjectsFocus: {
            main: scene.subjectsFocus?.main || "Primary subject",
            secondary: scene.subjectsFocus?.secondary || "Background elements",
            focus: scene.subjectsFocus?.focus || "Center"
          },
          intentImpactAnalysis: {
            creatorIntent: scene.intentImpactAnalysis?.creatorIntent || "Content creation",
            howExecuted: scene.intentImpactAnalysis?.howExecuted || "Standard techniques",
            viewerImpact: scene.intentImpactAnalysis?.viewerImpact || "Entertainment",
            narrativeSignificance: scene.intentImpactAnalysis?.narrativeSignificance || "Story progression"
          },
          textDialogue: {
            content: scene.textDialogue?.content || "No text available",
            style: scene.textDialogue?.style || "Standard"
          }
        };
      });
    }
    
    const duration = Date.now() - startTime;
    logWithTimestamp('‚úÖ Comprehensive analysis complete', { 
      duration: `${duration}ms`,
      sceneCount: result.scenes?.length || 0,
      hookCount: result.hooks?.length || 0,
      category: result.videoCategory?.category || 'unknown'
    });

    return result;

  } catch (error) {
    const duration = Date.now() - startTime;
    logWithTimestamp('‚ùå Comprehensive analysis failed', { 
      error: error.message,
      duration: `${duration}ms`
    });
    
    // Check if it's a rate limit or timeout issue
    const isRateLimit = error.message.includes('rate limit') || error.message.includes('Rate limit');
    const isTimeout = error.message.includes('timeout');
    
    let errorMessage = error.message;
    if (isRateLimit) {
      errorMessage = 'OpenAI rate limit exceeded. Please try again in a few minutes.';
    } else if (isTimeout) {
      errorMessage = 'Analysis timed out due to high API load. Please try again.';
    }
    
    // Return fallback structure with basic analysis
    return {
      videoCategory: { 
        category: 'entertainment', 
        confidence: 0.7, 
        subcategory: 'general content',
        reasoning: 'Fallback analysis due to API limitations'
      },
      scenes: [
        {
          sceneNumber: 1,
          timeRange: "0:00 - " + ((frameAnalyses.length / 1).toFixed(1)) + "s",
          title: "Complete Video",
          description: "Full video content - detailed analysis unavailable due to API limitations",
          duration: (frameAnalyses.length / 1).toFixed(1) + "s",
          framing: { shotTypes: ["Mixed"], cameraMovement: "Various", composition: "Standard" },
          lighting: { style: "Natural", mood: "Neutral", direction: "Front", quality: "Good" },
          mood: { emotional: "Neutral", atmosphere: "Standard", tone: "Casual" },
          actionMovement: { movement: "Various", direction: "Mixed", pace: "Medium" },
          audio: { music: "Background", soundDesign: "Standard", dialogue: audioAnalysis.transcription?.text || "No transcript" },
          visualEffects: { transitions: "Standard", effects: "Basic", graphics: "Text overlays" },
          settingEnvironment: { location: "Various", environment: "Mixed", background: "Standard" },
          subjectsFocus: { main: "Primary subject", secondary: "Background elements", focus: "Center" },
          intentImpactAnalysis: {
            creatorIntent: "Content creation",
            howExecuted: "Standard video production",
            viewerImpact: "Entertainment/Information",
            narrativeSignificance: "Complete message delivery"
          },
          textDialogue: { content: audioAnalysis.transcription?.text || "No text available", style: "Standard" }
        }
      ],
      hooks: [],
      contextualAnalysis: { 
        creatorIntent: { primaryIntent: 'entertainment', targetAudience: 'general', desiredAction: 'engagement' },
        messageDelivery: { coreMessage: 'Video content', deliveryMethod: 'visual storytelling' },
        themes: ['general content'],
        psychologicalAppeal: 'Visual engagement',
        socialContext: 'Social media content'
      },
      strategicOverview: {
        videoOverview: `Analysis temporarily unavailable due to API limitations (${errorMessage}). This appears to be social media content designed for engagement.`,
        whyItWorks: 'Visual storytelling and social media optimization',
        successFormula: 'Standard social media content structure',
        universalPrinciples: 'Engagement-focused content creation',
        technicalRequirements: 'Basic video production setup',
        viralPotential: 'Standard social media shareability'
      },
      contentStructure: `Basic video analysis completed. Detailed analysis unavailable: ${errorMessage}`,
      error: errorMessage,
      isLimitedAnalysis: true
    };
  }
}

async function generateStrategicOverview(scenes, audioAnalysis, contextualAnalysis, videoCategory) {
  const startTime = Date.now();
  logWithTimestamp('üìä Starting strategic overview generation');

  try {
    // Prepare comprehensive data for strategic analysis
    const sceneData = scenes.map(scene => ({
      title: scene.title,
      duration: scene.duration,
      description: scene.description,
      mood: scene.mood,
      dialogue: scene.dialogue,
      visualEffects: scene.visualEffects,
      contextualMeaning: scene.contextualMeaning
    }));

    const audioData = {
      transcript: audioAnalysis.transcription?.text || 'No dialogue available',
      musicAnalysis: audioAnalysis.musicAnalysis || { genre: 'Unknown', energy: 'Unknown' },
      audioHierarchy: audioAnalysis.audioHierarchy || { dialogue: [], musicLyrics: [], soundDesign: [] }
    };

    const strategicPrompt = `You are a professional content strategist analyzing this video to identify viral content patterns and provide replication frameworks.

SCENE-BY-SCENE DATA:
${sceneData.map((scene, i) => `
Scene ${i + 1}: ${scene.title} (${scene.duration})
- Description: ${scene.description}
- Mood: ${scene.mood?.emotional} / ${scene.mood?.atmosphere}
- Intent: ${scene.contextualMeaning?.intent || 'Not specified'}
- Execution: ${scene.contextualMeaning?.execution || 'Not specified'}
- Impact: ${scene.contextualMeaning?.impact || 'Not specified'}
- Text Content: ${scene.dialogue?.textContent || 'None'}
`).join('')}

AUDIO CONTEXT:
- Transcript: ${audioData.transcript}
- Music Genre: ${audioData.musicAnalysis.genre}
- Energy Level: ${audioData.musicAnalysis.energy}

CONTENT CATEGORY: ${videoCategory.category}
CONFIDENCE: ${Math.round(videoCategory.confidence * 100)}%

CONTEXTUAL INSIGHTS:
- Creator Intent: ${contextualAnalysis.creatorIntent?.primaryIntent || 'Not analyzed'}
- Target Audience: ${contextualAnalysis.targetAudience || 'General'}
- Core Message: ${contextualAnalysis.messageDelivery?.coreMessage || 'Not specified'}

Your task is to transform this technical analysis into a strategic content document following this structure:

## Video Overview
Brief summary of the video's content and primary purpose.

## Strategic Breakdown

### Why It Works: Universal Relatability
- Identify the core psychological/emotional appeal
- Explain what makes this universally relatable
- Connect to broader human experiences

### Success Formula: Replicable Structure
- Break down the step-by-step content structure
- Identify the key beats and timing
- Explain the progression logic

### Universal Principles: Adaptable Core Concepts  
- Extract the underlying success patterns
- Identify what can be adapted across industries
- Highlight the emotional journey mapping

### Technical Requirements: Execution Specifications
- List the essential production elements
- Identify what's critical vs. optional
- Provide complexity/resource assessment

## Replication Insights

### Implementation Framework
1. Step-by-step creation process
2. Key decision points and alternatives
3. Success metrics to watch for

### Adaptability Guidelines
- How to modify for different industries
- Platform-specific considerations  
- Audience targeting variations

### Resource Scaling
- Minimum viable version requirements
- Professional enhancement options
- Budget consideration factors

Focus on WHY techniques work, not just WHAT is happening. Explain the systematic patterns that make content effective and provide actionable replication strategies.`;

    const response = await handleRateLimit(async () => {
      return await callOpenAIWithRateLimit([
          {
            role: "system", 
            content: "You are a professional content strategist who transforms technical video analysis into actionable creative intelligence. Your analysis should help content creators understand not just what works, but why it works and how to adapt successful patterns to their own contexts."
          },
          {
            role: "user",
            content: strategicPrompt
          }
        ], 2000);
    });

    const strategicOverview = response.choices[0].message.content;
    
    const duration = Date.now() - startTime;
    logWithTimestamp('‚úÖ Strategic overview generation complete', { 
      duration: `${duration}ms`,
      contentLength: strategicOverview.length
    });

    return strategicOverview;

  } catch (error) {
    const duration = Date.now() - startTime;
    logWithTimestamp('‚ùå Strategic overview generation failed', { 
      error: error.message,
      duration: `${duration}ms`
    });
    
    // Return a fallback overview based on available data
    return `## Video Analysis Overview

**Content Type:** ${videoCategory.category}
**Duration:** ${scenes.length} scenes analyzed
**Primary Appeal:** ${contextualAnalysis.creatorIntent?.primaryIntent || 'Entertainment/Education'}

### Key Success Elements:
${scenes.map((scene, i) => `- Scene ${i + 1}: ${scene.contextualMeaning?.intent || scene.title}`).join('\n')}

### Replication Framework:
1. **Structure**: Follow the ${scenes.length}-scene progression shown
2. **Timing**: Maintain similar pacing (${scenes.map(s => s.duration).join(', ')})
3. **Mood Progression**: ${scenes.map(s => s.mood?.emotional).filter(Boolean).join(' ‚Üí ')}

*Note: Strategic analysis was limited due to processing constraints. Full analysis recommended for complete insights.*`;
  }
}

async function analyzeVideo(videoPath, userId = null, creditsToDeduct = null, requestId = 'unknown') {
  const startTime = Date.now();
  logWithTimestamp('üé¨ Starting complete video analysis', { videoPath });

  try {
  // Extract frames and audio
    logWithTimestamp('üîÑ Phase 1: Extracting frames and audio');
    await updateProgress(requestId, 'extraction', 2, 'Downloading and extracting frames from video...');
    const framesPromise = extractFrames(videoPath);
    const audioPromise = extractAudio(videoPath);
    
    const [frames, audioPath] = await Promise.all([framesPromise, audioPromise]);
    
    const extractionDuration = Date.now() - startTime;
    logWithTimestamp('‚úÖ Phase 1 complete: Extraction finished', { 
      frameCount: frames.length,
      audioPath,
      duration: `${extractionDuration}ms`
    });

    // Skip sequential processing - using parallel approach for 50% speed boost

    // üöÄ SPEED OPTIMIZATION: Run frame + audio analysis in parallel (50% faster)
    logWithTimestamp('üîÑ Phase 2-3: Parallel frame + audio analysis for 50% speed boost');
    await updateProgress(requestId, 'parallel_analysis', 5, `Starting parallel analysis for ${frames.length} frames + audio...`, { frameCount: frames.length });
    const parallelAnalysisStartTime = Date.now();
    
    const [frameAnalyses, audioAnalysis] = await Promise.all([
      analyzeFramesInBatches(frames, requestId),
      analyzeAudio(audioPath)
    ]);

    const parallelAnalysisDuration = Date.now() - parallelAnalysisStartTime;
    logWithTimestamp('‚úÖ Phase 2-3 complete: Parallel analysis finished', { 
      frameCount: frameAnalyses.length,
      duration: `${parallelAnalysisDuration}ms`,
      speedImprovement: 'Up to 50% faster than sequential processing'
    });

    // Clean up frames and audio
    logWithTimestamp('üßπ Phase 4: Cleaning up temporary files');
    await updateProgress(requestId, 'cleanup', 75, 'Cleaning up temporary files...');
    const cleanupStartTime = Date.now();
    
    frames.forEach((frame, index) => {
      try {
        fs.unlinkSync(frame);
        logWithTimestamp(`üóëÔ∏è Deleted frame ${index + 1}`, { frame });
      } catch (error) {
        logWithTimestamp(`‚ö†Ô∏è Failed to delete frame ${index + 1}`, { frame, error: error.message });
      }
    });
    
    try {
      fs.unlinkSync(audioPath);
      logWithTimestamp('üóëÔ∏è Deleted audio file', { audioPath });
    } catch (error) {
      logWithTimestamp('‚ö†Ô∏è Failed to delete audio file', { audioPath, error: error.message });
    }

    const cleanupDuration = Date.now() - cleanupStartTime;
    logWithTimestamp('‚úÖ Phase 4 complete: Cleanup finished', { 
      duration: `${cleanupDuration}ms`
    });

    // CONSOLIDATED ANALYSIS - Single API request replaces phases 5-7
    logWithTimestamp('üîÑ Phase 5: Comprehensive analysis (single request optimization)');
    await updateProgress(requestId, 'comprehensive_analysis', 80, 'Generating comprehensive scene and strategic analysis...');
    const comprehensiveAnalysisStartTime = Date.now();
    
    const comprehensiveResult = await generateComprehensiveAnalysis(frameAnalyses, audioAnalysis);
    
    const comprehensiveAnalysisDuration = Date.now() - comprehensiveAnalysisStartTime;
    const finalTotalDuration = Date.now() - startTime;
    
    await updateProgress(requestId, 'finalizing', 98, 'Finalizing analysis and preparing results...');
    
    logWithTimestamp('‚úÖ Phase 5 complete: Comprehensive analysis finished', { 
      duration: `${comprehensiveAnalysisDuration}ms`,
      sceneCount: comprehensiveResult.scenes?.length || 0,
      hookCount: comprehensiveResult.hooks?.length || 0,
      category: comprehensiveResult.videoCategory?.category || 'unknown'
    });

    const result = {
      contentStructure: comprehensiveResult.contentStructure,
      hook: extractHook(frameAnalyses[0]),
      totalDuration: `${(frames.length / 1).toFixed(1)}s`, // frames.length / 1fps = actual seconds
      scenes: comprehensiveResult.scenes,
      transcript: audioAnalysis.transcription || { text: 'No transcript available', segments: [] },
      hooks: comprehensiveResult.hooks,
      videoCategory: comprehensiveResult.videoCategory,
      contextualAnalysis: comprehensiveResult.contextualAnalysis,
      strategicOverview: comprehensiveResult.strategicOverview,
      videoMetadata: {
        totalFrames: frames.length,
        frameRate: 1, // 1 frame per second (optimized for speed)
        analysisTimestamp: new Date().toISOString()
      }
    };

    logWithTimestamp('üéâ Video analysis complete! (Optimized with 83% fewer API requests)', { 
      totalDuration: `${finalTotalDuration}ms`,
      frameCount: frames.length,
      sceneCount: comprehensiveResult.scenes?.length || 0,
      phases: {
        extraction: `${extractionDuration}ms`,
        frameAnalysis: `${frameAnalysisDuration}ms`,
        audioAnalysis: `${audioAnalysisDuration}ms`,
        cleanup: `${cleanupDuration}ms`,
        comprehensiveAnalysis: `${comprehensiveAnalysisDuration}ms`
      }
    });

    // Deduct user credits if applicable
    if (userId && isSupabaseAvailable()) {
      try {
        // Fallback compute credits if none provided
        if (!creditsToDeduct) {
          const seconds = parseFloat((result.totalDuration || '0').replace(/[^0-9.]/g, '')) || 0;
          creditsToDeduct = Math.max(1, Math.ceil(seconds / 15));
        }
        await updateUserCredits(userId, creditsToDeduct);
        logWithTimestamp('üí≥ Credits deducted', { userId, creditsToDeduct });
      } catch (credErr) {
        console.error('Failed to deduct credits', credErr);
      }
    }

    await updateProgress(requestId, 'complete', 100, 'Analysis complete!', {
      sceneCount: result.scenes?.length || 0,
      duration: result.totalDuration,
      category: result.videoCategory?.category
    });

    logWithTimestamp('üéâ Request completed successfully!', { 
      requestId,
      totalDuration: `${finalTotalDuration}ms`,
      analysisKeys: Object.keys(result),
      creditsDeducted: creditsToDeduct || null
    });

    return result;
  } catch (error) {
    const duration = Date.now() - startTime;
    logWithTimestamp('‚ùå Video analysis failed', { 
      error: error.message,
      stack: error.stack,
      duration: `${duration}ms`,
      videoPath
    });
    throw error;
  }
}

function extractShotType(analysis) {
  logWithTimestamp('üîç Extracting shot type', { analysisLength: analysis?.length || 0 });
  
  // Extract shot type from GPT-4 Vision analysis
  const shotTypes = ['close-up', 'medium shot', 'wide shot', 'extreme close-up', 'long shot'];
  const match = shotTypes.find(type => analysis.toLowerCase().includes(type));
  const result = match ? match.charAt(0).toUpperCase() + match.slice(1) : 'Medium Shot';
  
  logWithTimestamp('üìä Shot type extracted', { result, match });
  return result;
}

function extractDescription(analysis) {
  logWithTimestamp('üîç Extracting description', { analysisLength: analysis?.length || 0 });
  
  // Extract main visual elements and text overlays
  const result = analysis.split('\n')
    .filter(line => line.includes('visual elements') || line.includes('text overlay'))
    .join(' ');
    
  logWithTimestamp('üìä Description extracted', { resultLength: result.length });
  return result;
}

function detectEffects(frameAnalyses) {
  const startTime = Date.now();
  logWithTimestamp('üé® Detecting visual effects', { frameCount: frameAnalyses.length });
  
  const effects = [];
  
  frameAnalyses.forEach((analysis, index) => {
    if (analysis.toLowerCase().includes('effect') || analysis.toLowerCase().includes('transition')) {
      const effect = {
        name: 'Visual Effect',
        description: analysis.split('\n').find(line => line.toLowerCase().includes('effect'))?.trim() || 'Visual transition',
        timestamps: [`${index}s`]
      };
      effects.push(effect);
      logWithTimestamp(`üé® Effect detected at frame ${index + 1}`, effect);
    }
  });

  const duration = Date.now() - startTime;
  logWithTimestamp('‚úÖ Effect detection complete', { 
    effectCount: effects.length,
    duration: `${duration}ms`
  });

  return effects;
}

function parseMusicAnalysis(analysis) {
  logWithTimestamp('üéº Parsing music analysis', { analysisLength: analysis?.length || 0 });
  
  // Parse the GPT-4 analysis of audio
  const lines = analysis.split('\n');
  const result = {
    genre: lines.find(line => line.includes('genre'))?.split(':')[1]?.trim() || 'Background Music',
    bpm: parseInt(lines.find(line => line.includes('BPM'))?.split(':')[1]?.trim() || '120'),
    energy: lines.find(line => line.includes('Energy'))?.split(':')[1]?.trim() || 'Medium',
    mood: lines.find(line => line.includes('mood'))?.split(':')[1]?.trim() || 'Neutral'
  };
  
  logWithTimestamp('üéº Music analysis parsed', result);
  return result;
}

async function generateSceneAnalysis(frameAnalyses, audioAnalysis) {
  const startTime = Date.now();
  logWithTimestamp('üé¨ Starting scene detection and comprehensive analysis', { 
    frameCount: frameAnalyses.length 
  });

  try {
    // Step 1: Detect scene boundaries by analyzing continuity between frames
    const scenes = detectSceneBoundaries(frameAnalyses);
    logWithTimestamp('üîç Scene boundaries detected', { sceneCount: scenes.length });

    // Step 2: Generate comprehensive analysis for each scene
    const comprehensiveScenes = await Promise.all(
      scenes.map(async (scene, sceneIndex) => {
        return await generateSceneCard(scene, sceneIndex, audioAnalysis);
      })
    );

    const duration = Date.now() - startTime;
    logWithTimestamp('‚úÖ Scene analysis complete', {
      sceneCount: comprehensiveScenes.length,
      duration: `${duration}ms`
    });

    return comprehensiveScenes;
  } catch (error) {
    logWithTimestamp('‚ùå Scene analysis failed', { error: error.message });
    throw error;
  }
}

function detectSceneBoundaries(frameAnalyses) {
  logWithTimestamp('üîç Detecting scene boundaries', { 
    totalFrames: frameAnalyses.length,
    frameRate: '2fps (one analysis every 0.5s)'
  });
  
  const scenes = [];
  let currentScene = {
    startFrame: 0,
    frames: [frameAnalyses[0]]
  };

  for (let i = 1; i < frameAnalyses.length; i++) {
    const currentFrame = frameAnalyses[i];
    const previousFrame = frameAnalyses[i - 1];
    
    // Simple scene boundary detection based on significant changes
    const isNewScene = detectSceneChange(currentFrame.analysis, previousFrame.analysis);
    
    // Detailed decision logging
    const decisionReasons = [];
    if (isNewScene) {
      decisionReasons.push('CONTENT_CHANGE');
    }
    // Future: could push specific reasons from detectSceneChange if returned.
    logSceneDecision(i, isNewScene, decisionReasons);
    
    // If a significant change occurs, start a new scene. No arbitrary max-length cap.
    if (isNewScene && currentScene.frames.length >= 1) {
      // End current scene
      currentScene.endFrame = i - 1;
      currentScene.duration = currentScene.frames.length;
      scenes.push(currentScene);
      
      logWithTimestamp(`üé¨ Scene ${scenes.length} detected`, {
        startFrame: currentScene.startFrame,
        endFrame: currentScene.endFrame,
        duration: `${currentScene.duration * 0.5}s`,
        frameCount: currentScene.frames.length,
        reason: 'Content change detected'
      });
      
      // Start new scene
      currentScene = {
        startFrame: i,
        frames: [currentFrame]
      };
    } else {
      // Continue current scene
      currentScene.frames.push(currentFrame);
    }
  }

  // Add the last scene
  currentScene.endFrame = frameAnalyses.length - 1;
  currentScene.duration = currentScene.frames.length;
  scenes.push(currentScene);
  
  logWithTimestamp(`üé¨ Final scene ${scenes.length} added`, {
    startFrame: currentScene.startFrame,
    endFrame: currentScene.endFrame,
    duration: `${currentScene.duration * 0.5}s`,
    frameCount: currentScene.frames.length
  });

  logWithTimestamp('üìä Scene detection results', {
    totalScenes: scenes.length,
    avgSceneDuration: (scenes.reduce((sum, s) => sum + s.duration, 0) * 0.5 / scenes.length).toFixed(1) + 's',
    sceneBreakdown: scenes.map((s, i) => `Scene ${i+1}: ${(s.duration * 0.5).toFixed(1)}s`).join(', ')
  });

  return scenes;
}

function detectSceneChange(currentAnalysis, previousAnalysis) {
  // Normalize input to lowercase for easier comparison
  const currentText = currentAnalysis.toLowerCase();
  const previousText = previousAnalysis.toLowerCase();
  
  // ENHANCED: Detect SETTING / LOCATION changes (e.g., new environment)
  const settingChange = (
    currentText.includes('setting:') && previousText.includes('setting:') &&
    extractValue(currentText, 'setting:') !== extractValue(previousText, 'setting:')
  );
  
  // ENHANCED: Detect major FRAMING / CAMERA ANGLE changes
  const framingChange = (
    currentText.includes('framing:') && previousText.includes('framing:') &&
    extractValue(currentText, 'framing:') !== extractValue(previousText, 'framing:')
  );
  
  // ENHANCED: Detect SUBJECT changes (primary on-screen subjects swap)
  const subjectChange = (
    currentText.includes('subjects:') && previousText.includes('subjects:') &&
    !haveSimilarSubjects(extractValue(currentText, 'subjects:'), extractValue(previousText, 'subjects:'))
  );
  
  // ENHANCED: Detect ON-SCREEN TEXT changes (often signal punchlines / new context)
  const textOverlayChange = (
    // Handle various key names produced by LLM prompts
    ((currentText.includes('on_screen_text') && previousText.includes('on_screen_text') &&
      extractValue(currentText, 'on_screen_text:') !== extractValue(previousText, 'on_screen_text:')))
      ||
    ((currentText.includes('textcontent') && previousText.includes('textcontent') &&
      extractValue(currentText, 'textcontent:') !== extractValue(previousText, 'textcontent:')))
  );

  // ENHANCED: Detect AUDIO cue changes (music switch, silence, dialogue start)
  const audioCueChange = (
    currentText.includes('audio:') && previousText.includes('audio:') &&
    extractValue(currentText, 'audio:') !== extractValue(previousText, 'audio:')
  );

  // NEW: Detect NARRATIVE BEATS - story progression indicators
  const narrativeBeatChange = (
    // Dream/reality transitions
    (currentText.includes('dream') && !previousText.includes('dream')) ||
    (previousText.includes('dream') && !currentText.includes('dream')) ||
    (currentText.includes('reality') && !previousText.includes('reality')) ||
    (currentText.includes('wakes up') && !previousText.includes('wakes up')) ||
    (currentText.includes('sleeping') && !previousText.includes('sleeping')) ||
    
    // Emotional state changes  
    (currentText.includes('celebration') && !previousText.includes('celebration')) ||
    (currentText.includes('disbelief') && !previousText.includes('disbelief')) ||
    (currentText.includes('shock') && !previousText.includes('shock')) ||
    (currentText.includes('excitement') && !previousText.includes('excitement')) ||
    (currentText.includes('disappointed') && !previousText.includes('disappointed')) ||
    
    // Action progression keywords
    (currentText.includes('picks up') && !previousText.includes('picks up')) ||
    (currentText.includes('discovers') && !previousText.includes('discovers')) ||
    (currentText.includes('realizes') && !previousText.includes('realizes')) ||
    (currentText.includes('finds') && !previousText.includes('finds')) ||
    (currentText.includes('grabs') && !previousText.includes('grabs')) ||
    
    // Object-specific changes (for pee dream scenario)
    (currentText.includes('money') && !previousText.includes('money')) ||
    (currentText.includes('keys') && !previousText.includes('keys')) ||
    (currentText.includes('car') && !previousText.includes('car')) ||
    (currentText.includes('vehicle') && !previousText.includes('vehicle')) ||
    (currentText.includes('ground') && !previousText.includes('ground')) ||
    (currentText.includes('floor') && !previousText.includes('floor'))
  );

  // NEW: Detect VISUAL CONTRAST changes (color, brightness, style)
  const visualContrastChange = (
    // Color shifts
    (currentText.includes('color') && previousText.includes('color') &&
     extractValue(currentText, 'color') !== extractValue(previousText, 'color')) ||
    
    // Black & white vs color
    (currentText.includes('black') && currentText.includes('white') && 
     !(previousText.includes('black') && previousText.includes('white'))) ||
    (previousText.includes('black') && previousText.includes('white') && 
     !(currentText.includes('black') && currentText.includes('white'))) ||
     
    // Brightness changes indicating mood shift
    (currentText.includes('bright') && previousText.includes('dark')) ||
    (currentText.includes('dark') && previousText.includes('bright')) ||
    
    // Wet/dry state changes (for pee dream scenario)
    (currentText.includes('wet') && !previousText.includes('wet')) ||
    (currentText.includes('stain') && !previousText.includes('stain')) ||
    (currentText.includes('dry') && previousText.includes('wet'))
  );

  // NEW: Detect CONTEXTUAL MEANING shifts (story purpose changes)
  const contextualShift = (
    // Setup to payoff transition
    (previousText.includes('setup') && currentText.includes('payoff')) ||
    (previousText.includes('buildup') && currentText.includes('punchline')) ||
    
    // Irony reveals
    (currentText.includes('irony') && !previousText.includes('irony')) ||
    (currentText.includes('contrast') && !previousText.includes('contrast')) ||
    (currentText.includes('reveal') && !previousText.includes('reveal')) ||
    (currentText.includes('twist') && !previousText.includes('twist')) ||
    (currentText.includes('punchline') && !previousText.includes('punchline')) ||
    
    // Emotional tone shifts
    (currentText.includes('hopeful') && previousText.includes('disappointed')) ||
    (currentText.includes('disappointed') && previousText.includes('hopeful')) ||
    (currentText.includes('happy') && previousText.includes('sad')) ||
    (currentText.includes('sad') && previousText.includes('happy'))
  );

  // NEW: Detect ACTIVITY/ACTION changes
  const actionChange = (
    // Different verbs indicate new scene beats
    (currentText.includes('walking') && !previousText.includes('walking')) ||
    (currentText.includes('sitting') && !previousText.includes('sitting')) ||
    (currentText.includes('lying') && !previousText.includes('lying')) ||
    (currentText.includes('standing') && !previousText.includes('standing')) ||
    (currentText.includes('celebrating') && !previousText.includes('celebrating')) ||
    (currentText.includes('pointing') && !previousText.includes('pointing')) ||
    (currentText.includes('reaching') && !previousText.includes('reaching')) ||
    (currentText.includes('bending') && !previousText.includes('bending')) ||
    (currentText.includes('looking down') && !previousText.includes('looking down')) ||
    (currentText.includes('looking up') && !previousText.includes('looking up'))
  );

  // NEW: Detect LOCATION/SETTING changes
  const locationChange = (
    // Different locations
    (currentText.includes('bathroom') && !previousText.includes('bathroom')) ||
    (currentText.includes('car') && !previousText.includes('car')) ||
    (currentText.includes('street') && !previousText.includes('street')) ||
    (currentText.includes('indoor') && previousText.includes('outdoor')) ||
    (currentText.includes('outdoor') && previousText.includes('indoor')) ||
    (currentText.includes('wall') && !previousText.includes('wall')) ||
    (currentText.includes('bed') && !previousText.includes('bed')) ||
    (currentText.includes('room') && !previousText.includes('room'))
  );

  // A scene boundary occurs if ANY significant change happens - now much more sensitive
  const hasSceneChange = settingChange || framingChange || subjectChange || textOverlayChange || 
                        audioCueChange || narrativeBeatChange || visualContrastChange || 
                        contextualShift || actionChange || locationChange;
  
  // Log scene change detection for debugging
  if (hasSceneChange) {
    logWithTimestamp('üé¨ Scene change detected', {
      settingChange,
      framingChange, 
      subjectChange,
      textOverlayChange,
      audioCueChange,
      narrativeBeatChange,
      visualContrastChange,
      contextualShift,
      actionChange,
      locationChange
    });
  }
  
  return hasSceneChange;
}

function extractValue(text, key) {
  const regex = new RegExp(`${key}\\s*([^\\n]+)`, 'i');
  const match = text.match(regex);
  return match ? match[1].trim() : '';
}

function haveSimilarSubjects(current, previous) {
  if (!current || !previous) return false;
  
  const currentWords = current.toLowerCase().split(/\s+/);
  const previousWords = previous.toLowerCase().split(/\s+/);
  
  // Check if they share at least 30% of words
  const commonWords = currentWords.filter(word => previousWords.includes(word));
  return commonWords.length / Math.max(currentWords.length, previousWords.length) > 0.3;
}

async function generateSceneCard(scene, sceneIndex, audioAnalysis) {
  logWithTimestamp(`üé¨ Generating scene card ${sceneIndex + 1}`, {
    startFrame: scene.startFrame,
    endFrame: scene.endFrame,
    duration: scene.duration
  });

  // Aggregate frame analyses
  const frameData = scene.frames.map(f => f.analysis).join('\n\n');
  
  // Get audio segment for this scene
  const audioSegment = getAudioSegmentForScene(scene, audioAnalysis);
  
  // Generate comprehensive scene analysis including contextual meaning
  const sceneAnalysis = await handleRateLimit(async () => {
    const response = await openai.chat.completions.create({
      model: "gpt-4o",
      messages: [
        {
          role: "user",
          content: `Analyze this video scene (${scene.duration} seconds, frames ${scene.startFrame}-${scene.endFrame}) and create a comprehensive scene card with deep contextual understanding.

CRITICAL CONTEXT HIERARCHY:
1. DIALOGUE (highest priority): ${audioSegment.audioType === 'dialogue' ? audioSegment.transcription : 'No dialogue detected - focus on VISUAL story'}
2. ON-SCREEN TEXT (second priority): Extract ALL visible text from frame analysis
3. MUSIC/LYRICS (lower priority): ${audioSegment.audioType === 'music' ? audioSegment.transcription : 'Background music only'}

‚ö†Ô∏è IMPORTANT: If there is no actual dialogue (only background music/lyrics), focus ENTIRELY on the VISUAL story being told through actions, expressions, and on-screen elements.

CONTEXTUAL ANALYSIS FOCUS:
- What is the creator trying to achieve in this scene VISUALLY?
- HOW do they achieve their intent through VISUAL techniques?
- Why are certain VISUAL choices made?
- What impact does this scene have on the viewer through VISUALS?
- How does this scene contribute to the overall VISUAL narrative/message?
- What do on-screen text and visual elements reveal about the context?

Frame-by-frame analysis (pay special attention to ON_SCREEN_TEXT fields):
${frameData}

Audio context (Priority: ${audioSegment.priorityContext || 'Unknown'}):
Transcription: ${audioSegment.transcription}
Context Analysis: ${audioSegment.contextualAnalysis}
Audio Type: ${audioSegment.audioType}

Create a scene analysis card with the following structure (MUST match this exact format):

{
  "sceneNumber": ${sceneIndex + 1},
  "duration": "${(scene.duration * 0.5).toFixed(1)}s",
  "timeRange": "${(scene.startFrame * 0.5).toFixed(1)}s - ${(scene.endFrame * 0.5).toFixed(1)}s",
  "title": "[Brief descriptive title focusing on visual action/beat]",
  "description": "[What happens visually in this scene - focus on story progression]",
  "framing": {
    "shotTypes": ["[primary shot types used - e.g., close-up, medium shot, wide shot]"],
    "cameraMovement": "[static/pan/zoom/handheld/tracking/etc]",
    "composition": "[visual composition notes - rule of thirds, centered, etc]"
  },
  "lighting": {
    "style": "[natural/artificial/dramatic/mixed]",
    "mood": "[bright/dark/moody/soft/harsh]",
    "direction": "[front-lit/back-lit/side-lit/top-lit]",
    "quality": "[hard/soft/diffused/natural]"
  },
  "mood": {
    "emotional": "[happy/sad/tense/excited/confused/disappointed/etc]",
    "atmosphere": "[calm/energetic/mysterious/playful/dramatic]",
    "tone": "[serious/playful/dramatic/ironic/hopeful]"
  },
  "actionMovement": {
    "movement": "[description of physical actions and movements]",
    "direction": "[screen direction, eye lines, movement patterns]",
    "pace": "[slow/medium/fast/dynamic/static]"
  },
  "audio": {
    "music": "[music description or 'Background music' or 'None']",
    "soundDesign": "[sound effects, ambient sounds, or 'Standard']",
    "dialogue": "[spoken content or transcribed audio or 'None']"
  },
  "visualEffects": {
    "transitions": "[cuts/fades/wipes/dissolves/jump cuts]",
    "effects": "[filters, color grading, speed changes, overlays]",
    "graphics": "[text overlays, graphics, UI elements, typography]"
  },
  "settingEnvironment": {
    "location": "[specific location description - indoor/outdoor/bathroom/car/etc]",
    "environment": "[indoor/outdoor/studio/natural/urban/etc]",
    "background": "[detailed background elements and their significance]"
  },
  "subjectsFocus": {
    "main": "[primary subjects, people, objects of focus]",
    "secondary": "[supporting elements, background subjects]",
    "focus": "[what specifically draws viewer attention and why]"
  },
  "intentImpactAnalysis": {
    "creatorIntent": "[what the creator is trying to achieve in this specific scene]",
    "howExecuted": "[specific techniques and methods used to achieve the intent]",
    "viewerImpact": "[expected psychological and emotional effect on viewers]",
    "narrativeSignificance": "[how this scene contributes to the overall story/message]"
  },
  "textDialogue": {
    "content": "[exact text content visible on screen or spoken dialogue]",
    "style": "[font style, overlay treatment, visual presentation]"
  }
}

CRITICAL: Provide only the JSON response with no additional text. Ensure all fields are filled with meaningful content based on the visual analysis.`
        }
      ],
      max_tokens: 1500
    });

    return response.choices[0].message.content;
  });

  try {
    // Parse the JSON response
    const sceneCard = JSON.parse(sceneAnalysis);
    
    // Ensure all required properties exist with defaults
    const safeSceneCard = {
      sceneNumber: sceneCard.sceneNumber || sceneIndex + 1,
      duration: sceneCard.duration || `${(scene.duration * 0.5).toFixed(1)}s`,
      timeRange: sceneCard.timeRange || `${(scene.startFrame * 0.5).toFixed(1)}s - ${(scene.endFrame * 0.5).toFixed(1)}s`,
      title: sceneCard.title || `Scene ${sceneIndex + 1}`,
      description: sceneCard.description || "Scene analysis in progress",
      framing: {
        shotTypes: sceneCard.framing?.shotTypes || ["Medium shot"],
        cameraMovement: sceneCard.framing?.cameraMovement || "Static",
        composition: sceneCard.framing?.composition || "Standard"
      },
      lighting: {
        style: sceneCard.lighting?.style || "Natural",
        mood: sceneCard.lighting?.mood || "Neutral",
        direction: sceneCard.lighting?.direction || "Front-lit",
        quality: sceneCard.lighting?.quality || "Soft"
      },
      mood: {
        emotional: sceneCard.mood?.emotional || "Neutral",
        atmosphere: sceneCard.mood?.atmosphere || "Calm",
        tone: sceneCard.mood?.tone || "Casual"
      },
      actionMovement: {
        movement: sceneCard.actionMovement?.movement || "Various actions",
        direction: sceneCard.actionMovement?.direction || "Mixed",
        pace: sceneCard.actionMovement?.pace || "Medium"
      },
      audio: {
        music: sceneCard.audio?.music || "Background music",
        soundDesign: sceneCard.audio?.soundDesign || "Standard",
        dialogue: sceneCard.audio?.dialogue || "None"
      },
      visualEffects: {
        transitions: sceneCard.visualEffects?.transitions || "Standard cuts",
        effects: sceneCard.visualEffects?.effects || "Basic",
        graphics: sceneCard.visualEffects?.graphics || "Text overlays"
      },
      settingEnvironment: {
        location: sceneCard.settingEnvironment?.location || "Various",
        environment: sceneCard.settingEnvironment?.environment || "Mixed",
        background: sceneCard.settingEnvironment?.background || "Standard"
      },
      subjectsFocus: {
        main: sceneCard.subjectsFocus?.main || "Primary subject",
        secondary: sceneCard.subjectsFocus?.secondary || "Background elements",
        focus: sceneCard.subjectsFocus?.focus || "Center"
      },
      intentImpactAnalysis: {
        creatorIntent: sceneCard.intentImpactAnalysis?.creatorIntent || "Content creation",
        howExecuted: sceneCard.intentImpactAnalysis?.howExecuted || "Standard techniques",
        viewerImpact: sceneCard.intentImpactAnalysis?.viewerImpact || "Entertainment",
        narrativeSignificance: sceneCard.intentImpactAnalysis?.narrativeSignificance || "Story progression"
      },
      textDialogue: {
        content: sceneCard.textDialogue?.content || "No text available",
        style: sceneCard.textDialogue?.style || "Standard"
      }
    };
    
    logWithTimestamp(`‚úÖ Scene card ${sceneIndex + 1} generated successfully`);
    return safeSceneCard;
  } catch (parseError) {
    logWithTimestamp(`‚ö†Ô∏è Failed to parse scene card JSON for scene ${sceneIndex + 1}`, {
      error: parseError.message
    });
    
    // Return a fallback structure
    return {
      sceneNumber: sceneIndex + 1,
      duration: `${scene.duration}s`,
      timeRange: `${scene.startFrame}s - ${scene.endFrame}s`,
      title: `Scene ${sceneIndex + 1}`,
      description: "Scene analysis failed to parse",
      error: parseError.message,
      rawAnalysis: sceneAnalysis
    };
  }
}

function getAudioSegmentForScene(scene, audioAnalysis) {
  if (!audioAnalysis?.transcription?.segments) {
    return audioAnalysis?.analysis || 'No audio analysis available';
  }

  // Find audio segments that overlap with this scene (convert to 0.5s intervals for 2fps)
  const sceneStartTime = scene.startFrame * 0.5;
  const sceneEndTime = scene.endFrame * 0.5;
  
  const relevantSegments = audioAnalysis.transcription.segments.filter(segment => {
    const segStart = segment.start || 0;
    const segEnd = segment.end || 0;
    return (segStart >= sceneStartTime && segStart <= sceneEndTime) || 
           (segEnd >= sceneStartTime && segEnd <= sceneEndTime) ||
           (segStart <= sceneStartTime && segEnd >= sceneEndTime);
  });

  if (relevantSegments.length === 0) {
    return {
      transcription: 'No audio detected for this scene',
      contextualAnalysis: audioAnalysis?.separatedAudio?.audioSummary || 'Limited audio context',
      audioType: 'none'
    };
  }

  // Prioritize dialogue over music for context
  const sceneTranscript = relevantSegments.map(seg => seg.text).join(' ');
  
  // Determine if this scene primarily contains dialogue or music
  let audioType = 'mixed';
  let contextualMeaning = '';
  
  if (audioAnalysis.separatedAudio) {
    const dialogueContent = audioAnalysis.separatedAudio.dialogue?.content || '';
    const musicContent = audioAnalysis.separatedAudio.musicLyrics?.content || '';
    
    // Check if scene transcript is primarily dialogue
    if (dialogueContent && sceneTranscript && dialogueContent.includes(sceneTranscript.substring(0, 50))) {
      audioType = 'dialogue';
      contextualMeaning = audioAnalysis.separatedAudio.dialogue?.primaryContext || 'Dialogue provides key context';
    } else if (musicContent && sceneTranscript && musicContent.includes(sceneTranscript.substring(0, 50))) {
      audioType = 'music';
      contextualMeaning = audioAnalysis.separatedAudio.musicLyrics?.role || 'Music supports the mood';
    } else {
      contextualMeaning = audioAnalysis.separatedAudio.audioSummary || 'Mixed audio content';
    }
  }

  return {
    transcription: sceneTranscript,
    contextualAnalysis: contextualMeaning,
    audioType: audioType,
    musicAnalysis: audioAnalysis.analysis,
    priorityContext: audioType === 'dialogue' ? 'HIGH - Contains spoken dialogue' : 'MEDIUM - Music/effects only'
  };
}

async function generateContentStructure(frameAnalyses, audioAnalysis, scenes) {
  const startTime = Date.now();
  logWithTimestamp('üìù Generating strategic content analysis', { 
    frameCount: frameAnalyses.length,
    sceneCount: scenes.length,
    hasAudioAnalysis: !!audioAnalysis?.analysis
  });
  
  try {
    // Compile comprehensive data for strategic analysis
    const videoLength = (frameAnalyses.length / 2).toFixed(1); // 2fps = 0.5s per frame
    
    // Extract all on-screen text from frames
    const allFrameText = frameAnalyses.map(frame => {
      const textMatch = frame.analysis.match(/ON_SCREEN_TEXT:\s*([^-\n]+)/i);
      return textMatch ? textMatch[1].trim() : '';
    }).filter(text => text && text !== 'None' && text !== 'No visible text');
    
    // Get dialogue vs music separation
    const dialogueContent = audioAnalysis.separatedAudio?.dialogue?.content || '';
    const musicContent = audioAnalysis.separatedAudio?.musicLyrics?.content || '';
    const contextPriority = audioAnalysis.separatedAudio?.contextPriority || 'unknown';
    
    // Compile scene progression
    const sceneProgression = scenes.map((scene, index) => ({
      sceneNumber: index + 1,
      title: scene.title || `Scene ${index + 1}`,
      timeRange: scene.timeRange || `${scene.startFrame * 0.5}s-${scene.endFrame * 0.5}s`,
      contextualMeaning: scene.contextualMeaning || {}
    }));

    const strategicAnalysis = await handleRateLimit(async () => {
      const response = await openai.chat.completions.create({
        model: "gpt-4o",
        messages: [
          {
            role: "user",
            content: `Create a strategic video analysis overview in the format of a professional content strategist. Analyze this ${videoLength}-second video for its deeper meaning, viral potential, and systematic structure.

VIDEO DATA:
Duration: ${videoLength} seconds
Scenes: ${scenes.length}
Context Priority: ${contextPriority}

DIALOGUE/NARRATION: "${dialogueContent}"
ON-SCREEN TEXT: ${allFrameText.join(' | ')}
MUSIC/LYRICS: "${musicContent}"

SCENE PROGRESSION:
${sceneProgression.map(scene => `Scene ${scene.sceneNumber}: ${scene.title} (${scene.timeRange})`).join('\n')}

FRAME-BY-FRAME ANALYSIS:
${frameAnalyses.slice(0, 8).map((frame, i) => `${(i * 0.5).toFixed(1)}s: ${frame.contextualMeaning || 'Context analysis available'}`).join('\n')}

Create a comprehensive analysis following this structure:

**VIDEO OVERVIEW**
- Format/Genre classification
- Primary message/theme
- Emotional arc (start ‚Üí middle ‚Üí end)
- Why this video works (3-4 key reasons)

**STRATEGIC BREAKDOWN**
- Core message structure
- Escalation pattern (if applicable)
- Narrative techniques used
- Visual storytelling evolution

**REPLICATION INSIGHTS**
- Template/formula identification
- Key success elements
- Adaptable patterns
- Viral mechanics

Focus on WHY this content works, not just WHAT happens. Identify the systematic approach that makes it effective and replicable. Write as if training someone to create similar content.`
          }
        ],
        max_tokens: 8000
      });

      return response.choices[0].message.content;
    });

    const duration = Date.now() - startTime;
    logWithTimestamp('‚úÖ Strategic content analysis complete', { 
      analysisLength: strategicAnalysis?.length || 0,
      duration: `${duration}ms`
    });

    return strategicAnalysis;

  } catch (error) {
    const duration = Date.now() - startTime;
    logWithTimestamp('‚ùå Strategic content analysis failed', { 
      error: error.message,
      duration: `${duration}ms`
    });
    
    // Fallback to basic structure
    const videoLength = (frameAnalyses.length / 2).toFixed(1);
    return `${videoLength}-second video with ${scenes.length} scenes. Strategic analysis failed: ${error.message}`;
  }
}

function extractMultipleValues(text, key) {
  const regex = new RegExp(`${key}\\s*([^\\n]+)`, 'gi');
  const matches = [...text.matchAll(regex)];
  return [...new Set(matches.map(match => match[1].trim()))].slice(0, 5); // Limit to 5 unique values
}

function extractHook(firstFrameAnalysis) {
  logWithTimestamp('üéØ Extracting hook from first frame', { 
    analysisType: typeof firstFrameAnalysis,
    analysisLength: firstFrameAnalysis?.analysis?.length || firstFrameAnalysis?.length || 0 
  });
  
  // Handle both old string format and new object format
  let analysisText = '';
  if (typeof firstFrameAnalysis === 'string') {
    analysisText = firstFrameAnalysis;
  } else if (firstFrameAnalysis?.analysis) {
    analysisText = firstFrameAnalysis.analysis;
  } else {
    logWithTimestamp('‚ö†Ô∏è No valid analysis found for hook extraction');
    return 'Engaging opening';
  }
  
  // Extract hook from the first frame's analysis
  const lines = analysisText.split('\n');
  const actionLine = lines.find(line => line.toLowerCase().includes('action:'));
  const subjectLine = lines.find(line => line.toLowerCase().includes('subjects:'));
  
  let result = 'Engaging opening';
  if (actionLine) {
    result = actionLine.split(':')[1]?.trim() || result;
  } else if (subjectLine) {
    result = `Opening featuring ${subjectLine.split(':')[1]?.trim()}`;
  }
  
  logWithTimestamp('üéØ Hook extracted', { result, actionLine, subjectLine });
  return result;
}

// Extract video hooks from frame analyses and audio
async function extractVideoHooks(frameAnalyses, audioAnalysis) {
  logWithTimestamp('üîç Analyzing video for hooks...', {
    frameCount: frameAnalyses?.length || 0,
    hasAudio: !!audioAnalysis,
    hasTranscript: !!audioAnalysis?.transcription?.text
  });
  
  try {
    const hookPrompt = `Analyze this video data to identify attention-grabbing hooks. Look for:

VISUAL DISRUPTERS:
- Camera movements (zooms, pans, tilts, shakes, whip pans, speed ramps)
- Zoom punches (rapid zoom in/out for emphasis)
- Speed ramps (slow motion to normal speed transitions)
- Gimbal movements (smooth tracking/following shots)
- Match cuts and creative transitions
- Interesting objects or props appearing
- Actions by talent (gestures, movements, expressions)
- Scene changes or abrupt transitions
- Visual effects, graphics, or overlays
- Rack focus (focus pulling between subjects)

QUESTIONS:
- Direct questions to the viewer
- Breaking the 4th wall moments
- Rhetorical questions that engage

ACTION STATEMENTS:
- Positive statements ("Do these 5 things if you want X")
- Negative statements ("Avoid these if you want to get X")
- Commands or calls to action
- Lists or numbered points

TRANSCRIPT: ${audioAnalysis.transcription?.text || 'No audio transcript available'}

FRAME DATA: ${frameAnalyses.slice(0, 10).map((frame, i) => `Frame ${i+1}s: ${typeof frame === 'string' ? frame : frame.analysis || JSON.stringify(frame)}`).join('\n')}

Return a JSON array of hooks found, each with:
{
  "timestamp": "Xs",
  "type": "visual_disrupter|question|positive_statement|negative_statement",
  "description": "What specifically happens",
  "impact": "high|medium|low",
  "element": "Specific visual or audio element"
}`;

    const response = await handleRateLimit(async () => {
      return await callOpenAIWithRateLimit([
          {
            role: "user",
            content: hookPrompt
          }
        ], 1000);
    });

    const hooksText = response.choices[0].message.content;
    logWithTimestamp('ü§ñ AI hooks response', { 
      responseLength: hooksText?.length || 0,
      response: hooksText?.substring(0, 300) // Log first 300 chars
    });
    
    // Try to parse JSON, fallback to empty array if parsing fails
    try {
      const hooks = JSON.parse(hooksText);
      const validHooks = Array.isArray(hooks) ? hooks : [];
      logWithTimestamp('‚úÖ Hooks parsed successfully', { hookCount: validHooks.length });
      return validHooks;
    } catch (parseError) {
      logWithTimestamp('‚ö†Ô∏è Failed to parse hooks JSON, extracting manually', { 
        error: parseError.message,
        rawResponse: hooksText?.substring(0, 500)
      });
      return extractHooksFromText(hooksText);
    }
    
  } catch (error) {
    logWithTimestamp('‚ùå Error extracting hooks:', error.message);
    return [];
  }
}

// Fallback hook extraction from text
function extractHooksFromText(text) {
  const hooks = [];
  const lines = text.split('\n');
  
  lines.forEach(line => {
    if (line.includes('timestamp') || line.includes('Frame')) {
      const timestampMatch = line.match(/(\d+)s/);
      if (timestampMatch) {
        hooks.push({
          timestamp: `${timestampMatch[1]}s`,
          type: 'visual_disrupter',
          description: line.trim(),
          impact: 'medium',
          element: 'Detected from analysis'
        });
      }
    }
  });
  
  return hooks;
}

// Categorize video into one of the 7 categories
async function categorizeVideo(frameAnalyses, audioAnalysis, scenes) {
  logWithTimestamp('üè∑Ô∏è Categorizing video...', {
    frameCount: frameAnalyses?.length || 0,
    hasAudio: !!audioAnalysis,
    hasTranscript: !!audioAnalysis?.transcription?.text,
    sceneCount: scenes?.length || 0
  });
  
  try {
    const categoryPrompt = `Analyze this video content and categorize it into ONE of these 7 categories:

1. DELIGHTFUL MESSAGING
- Surprising/delightful visual or narrative twist
- "You got me" scroll-stopping moment
- Quick pivot to brand message
- Emotional resonance and shareability

2. ENGAGING EDUCATION  
- 2-4 clear educational points
- Mini-disrupters with each point
- Energetic pacing
- Authoritative educator positioning

3. DYNAMIC B-ROLL
- Visually striking footage
- Creative transitions (match cuts, speed ramps)
- Hyperlapses, gimbal moves, camera motion
- Close-ups on textures and movement

4. SITUATIONAL CREATIVE
- Unexpected, relatable context
- Pop-culture analogies or skits
- Subverted expectations
- Brand personality showcase

5. NARRATED NARRATIVE
- Fly-on-the-wall style
- Wide/static shots with voice-over
- Internal monologue style
- Intimate and authentic feel

6. BTS INTERVIEW
- Candid, in-action insights
- Interview while working
- Raw and authentic feel
- Trust-building content

7. TUTORIAL
- Step-by-step instructional content
- Process demonstration and explanation
- "How to" or "Follow along" format
- Clear sequential steps with visual demonstration
- Educational but focused on practical skills/techniques

TRANSCRIPT: ${audioAnalysis.transcription?.text || 'No audio transcript available'}

SCENES: ${scenes.map(scene => `Scene ${scene.sceneNumber}: ${scene.description}`).join('\n')}

VISUAL ELEMENTS: ${frameAnalyses.slice(0, 5).map((frame, i) => `${i+1}s: ${typeof frame === 'string' ? frame.substring(0, 200) : (frame.analysis || JSON.stringify(frame)).substring(0, 200)}`).join('\n')}

Return JSON with:
{
  "category": "delightful_messaging|engaging_education|dynamic_broll|situational_creative|narrated_narrative|bts_interview|tutorial",
  "confidence": 0.0-1.0,
  "reasoning": "Why this category fits",
  "keyIndicators": ["indicator1", "indicator2", "indicator3"]
}`;

    const response = await handleRateLimit(async () => {
      return await callOpenAIWithRateLimit([
          {
            role: "user",
            content: categoryPrompt
          }
        ], 500);
    });

    const categoryText = response.choices[0].message.content;
    logWithTimestamp('ü§ñ AI categorization response', { 
      responseLength: categoryText?.length || 0,
      response: categoryText?.substring(0, 500) // Log first 500 chars
    });
    
    try {
      const parsed = JSON.parse(categoryText);
      logWithTimestamp('‚úÖ Category parsed successfully', { category: parsed.category, confidence: parsed.confidence });
      return parsed;
    } catch (parseError) {
      logWithTimestamp('‚ö†Ô∏è Failed to parse category JSON, using fallback', { 
        error: parseError.message,
        rawResponse: categoryText 
      });
      return {
        category: 'dynamic_broll',
        confidence: 0.5,
        reasoning: 'Unable to parse AI response, defaulting to dynamic b-roll',
        keyIndicators: ['Visual content detected']
      };
    }
    
  } catch (error) {
    logWithTimestamp('‚ùå Error categorizing video:', { 
      error: error.message,
      stack: error.stack
    });
    return {
      category: 'dynamic_broll',
      confidence: 0.0,
      reasoning: `Error during categorization: ${error.message}`,
      keyIndicators: ['Analysis failed']
    };
  }
}

// Analyze the deeper context, narrative, and subtle messaging of the video
async function analyzeVideoContext(frameAnalyses, audioAnalysis, scenes) {
  logWithTimestamp('üß† Analyzing video context and narrative...', {
    frameCount: frameAnalyses?.length || 0,
    hasAudio: !!audioAnalysis,
    hasTranscript: !!audioAnalysis?.transcription?.text,
    sceneCount: scenes?.length || 0
  });
  
  try {
    const contextPrompt = `Analyze this video's deeper context, narrative, and subtle messaging. Look beyond surface-level content to understand:

INTENT & EXECUTION ANALYSIS:
- What is the creator's specific intent (humor, education, persuasion, etc.)?
- HOW do they achieve this intent? What specific techniques are used?
- What narrative devices, timing, or visual tricks create the desired effect?
- What makes it effective or impactful?

HUMOR MECHANICS (if applicable):
- What specifically makes this funny? (timing, contrast, expectation subversion, irony)
- Are there visual gags, text reveals, timing cuts, or freeze frames?
- How does timing contribute to the humor?
- What expectations are set up and then broken?
- Is it using contradiction, exaggeration, or relatable scenarios?

NARRATIVE STRUCTURE:
- How is the story structured to maximize impact?
- What is the setup, conflict, and resolution/punchline?
- How do visual and audio elements work together?
- What storytelling techniques enhance the message?

VISUAL STORYTELLING TECHNIQUES:
- How do on-screen text, graphics, and timing create meaning?
- What role do freeze frames, transitions, or cuts play?
- How do facial expressions, body language, and reactions contribute?
- What visual contrasts or juxtapositions are used?

CHARACTER DYNAMICS:
- Who are the main subjects and what do they represent?
- How do their situations, reactions, and outcomes convey the message?
- What stereotypes or archetypes are being utilized?
- How do character interactions drive the narrative?

MESSAGE DELIVERY:
- What is the core message and how is it communicated?
- What techniques make the message memorable or shareable?
- How does the creator ensure the audience "gets it"?
- What assumptions about audience knowledge are made?

TRANSCRIPT: ${audioAnalysis.transcription?.text || 'No audio transcript available'}

SCENES SUMMARY: ${scenes.map(scene => `Scene ${scene.sceneNumber}: ${scene.description} (${scene.duration})`).join('\n')}

VISUAL PROGRESSION: ${frameAnalyses.slice(0, 8).map((frame, i) => `${(i * 0.5).toFixed(1)}s: ${typeof frame === 'string' ? frame.substring(0, 150) : (frame.analysis || JSON.stringify(frame)).substring(0, 150)}`).join('\n')}

Return detailed JSON analysis:
{
  "creatorIntent": {
    "primaryIntent": "What the creator wants to achieve",
    "howAchieved": "Specific techniques and methods used",
    "effectivenessFactors": ["factor1", "factor2", "factor3"]
  },
  "humorMechanics": {
    "isHumorous": true/false,
    "humorType": "timing|irony|contrast|exaggeration|relatability|visual_gag",
    "specificTechniques": ["technique1", "technique2"],
    "setupAndPayoff": "How expectations are set and subverted",
    "timingElements": "How timing creates the effect"
  },
  "narrativeStructure": {
    "setup": "Initial situation or premise",
    "conflict": "Problem or tension introduced",
    "resolution": "How it's resolved or punchline delivered",
    "storytellingDevices": ["device1", "device2"]
  },
  "visualTechniques": {
    "textElements": "How on-screen text contributes",
    "visualEffects": "Freeze frames, transitions, cuts",
    "facialExpressions": "How expressions convey meaning",
    "visualContrasts": "Before/after, comparison shots"
  },
  "characters": [
    {
      "description": "Character description",
      "represents": "What they symbolize",
      "narrative_function": "Their role in achieving the intent",
      "key_moments": "Important actions or reactions"
    }
  ],
  "messageDelivery": {
    "coreMessage": "The main point being made",
    "deliveryMethod": "How the message is communicated",
    "memorabilityFactors": ["factor1", "factor2"],
    "audienceAssumptions": ["assumption1", "assumption2"]
  },
  "contextType": "humor|comparison|tutorial|story|commentary|parody",
  "targetAudience": "Who this is made for",
  "keyInsights": [
    "Specific insight about how intent is achieved",
    "Technical insight about the methods used",
    "Strategic insight about why it works"
  ]
}`;

    const response = await handleRateLimit(async () => {
      return await callOpenAIWithRateLimit([
          {
            role: "user",
            content: contextPrompt
          }
        ], 1500);
    });

    const contextText = response.choices[0].message.content;
    logWithTimestamp('ü§ñ AI context analysis response', { 
      responseLength: contextText?.length || 0,
      response: contextText?.substring(0, 500) // Log first 500 chars
    });
    
    try {
      const parsed = JSON.parse(contextText);
      logWithTimestamp('‚úÖ Context analysis parsed successfully', { 
        narrative: parsed.mainNarrative?.substring(0, 100),
        themeCount: parsed.themes?.length || 0,
        characterCount: parsed.characters?.length || 0,
        contextType: parsed.contextType
      });
      return parsed;
    } catch (parseError) {
      logWithTimestamp('‚ö†Ô∏è Failed to parse context JSON, using fallback', { 
        error: parseError.message,
        rawResponse: contextText?.substring(0, 500)
      });
      return extractContextFromText(contextText);
    }
    
  } catch (error) {
    logWithTimestamp('‚ùå Error analyzing video context:', { 
      error: error.message,
      stack: error.stack
    });
    return {
      creatorIntent: {
        primaryIntent: 'Context analysis failed',
        howAchieved: 'Unable to analyze techniques',
        effectivenessFactors: ['Analysis error']
      },
      humorMechanics: {
        isHumorous: false,
        humorType: 'unknown',
        specificTechniques: ['Analysis failed'],
        setupAndPayoff: 'Unable to analyze',
        timingElements: 'Not available'
      },
      narrativeStructure: {
        setup: 'Analysis failed',
        conflict: 'Analysis failed',
        resolution: 'Analysis failed',
        storytellingDevices: ['Error in analysis']
      },
      visualTechniques: {
        textElements: 'Analysis failed',
        visualEffects: 'Analysis failed',
        facialExpressions: 'Analysis failed',
        visualContrasts: 'Analysis failed'
      },
      characters: [],
      messageDelivery: {
        coreMessage: 'Unable to analyze context',
        deliveryMethod: 'Analysis failed',
        memorabilityFactors: ['Error in analysis'],
        audienceAssumptions: ['Unknown']
      },
      contextType: 'unknown',
      targetAudience: 'Unknown',
      keyInsights: [`Error during analysis: ${error.message}`]
    };
  }
}

// Fallback context extraction from text
function extractContextFromText(text) {
  const lines = text.split('\n');
  
  // Try to extract key information from the text response
  const narrative = lines.find(line => line.toLowerCase().includes('narrative') || line.toLowerCase().includes('story'))?.trim() || 'Unable to parse narrative';
  const themes = lines.filter(line => line.toLowerCase().includes('theme')).map(line => line.trim()).slice(0, 3);
  
  return {
    creatorIntent: {
      primaryIntent: 'Intent analysis failed',
      howAchieved: 'Unable to parse specific techniques',
      effectivenessFactors: ['Fallback analysis']
    },
    humorMechanics: {
      isHumorous: false,
      humorType: 'unknown',
      specificTechniques: ['Unable to analyze'],
      setupAndPayoff: 'Analysis failed',
      timingElements: 'Not available'
    },
    narrativeStructure: {
      setup: 'Unable to parse setup',
      conflict: 'Unable to parse conflict',
      resolution: 'Unable to parse resolution',
      storytellingDevices: ['Fallback analysis']
    },
    visualTechniques: {
      textElements: 'Analysis failed',
      visualEffects: 'Analysis failed',
      facialExpressions: 'Analysis failed',
      visualContrasts: 'Analysis failed'
    },
    characters: [{
      description: 'Characters detected in video',
      represents: 'Various roles',
      narrative_function: 'Unable to analyze',
      key_moments: 'Analysis failed'
    }],
    messageDelivery: {
      coreMessage: 'Complex messaging detected',
      deliveryMethod: 'Multiple techniques',
      memorabilityFactors: ['Fallback analysis'],
      audienceAssumptions: ['General audience']
    },
    contextType: 'comparison',
    targetAudience: 'Content creators and tool users',
    keyInsights: ['Fallback analysis - check logs for details']
  };
}

async function cleanupFile(filePath) {
  const startTime = Date.now();
  logWithTimestamp('üßπ Cleaning up file', { filePath });
  
  try {
    if (fs.existsSync(filePath)) {
      const stats = fs.statSync(filePath);
      fs.unlinkSync(filePath);
      const duration = Date.now() - startTime;
      logWithTimestamp('‚úÖ File cleanup successful', { 
        filePath,
        fileSize: `${(stats.size / 1024 / 1024).toFixed(2)} MB`,
        duration: `${duration}ms`
      });
    } else {
      logWithTimestamp('‚ÑπÔ∏è File does not exist, skipping cleanup', { filePath });
    }
  } catch (error) {
    const duration = Date.now() - startTime;
    logWithTimestamp('‚ùå File cleanup failed', { 
      filePath,
      error: error.message,
      duration: `${duration}ms`
    });
  }
}

// Token usage tracking and rate limiting
class TokenTracker {
  constructor() {
    this.tokenUsage = [];
    // Updated for new OpenAI limits - using gpt-4o-mini (2M TPM, 5K RPM)
    this.maxTokensPerMinute = 1800000; // 90% of 2M TPM for safety
    this.requestsPerMinute = 4500; // 90% of 5K RPM for safety
    this.requests = [];
  }

  // Add token usage with timestamp
  addTokenUsage(tokens) {
    const now = Date.now();
    this.tokenUsage.push({ tokens, timestamp: now });
    this.requests.push({ timestamp: now });
    
    // Clean up entries older than 1 minute
    this.cleanupOldEntries();
  }

  // Remove entries older than 1 minute
  cleanupOldEntries() {
    const oneMinuteAgo = Date.now() - 60000;
    this.tokenUsage = this.tokenUsage.filter(entry => entry.timestamp > oneMinuteAgo);
    this.requests = this.requests.filter(entry => entry.timestamp > oneMinuteAgo);
  }

  // Get current token usage in the last minute
  getCurrentTokenUsage() {
    this.cleanupOldEntries();
    return this.tokenUsage.reduce((total, entry) => total + entry.tokens, 0);
  }

  // Get current request count in the last minute
  getCurrentRequestCount() {
    this.cleanupOldEntries();
    return this.requests.length;
  }

  // Check if we can make a request with estimated tokens
  canMakeRequest(estimatedTokens = 5000) {
    const currentTokens = this.getCurrentTokenUsage();
    const currentRequests = this.getCurrentRequestCount();
    
    return (currentTokens + estimatedTokens) <= this.maxTokensPerMinute && 
           currentRequests < this.requestsPerMinute;
  }

  // Calculate how long to wait before making next request
  getWaitTime(estimatedTokens = 5000) {
    const currentTokens = this.getCurrentTokenUsage();
    const currentRequests = this.getCurrentRequestCount();
    
    if (currentTokens + estimatedTokens > this.maxTokensPerMinute) {
      // Find the oldest token usage that would put us under the limit
      let tokensToRemove = (currentTokens + estimatedTokens) - this.maxTokensPerMinute;
      for (let i = 0; i < this.tokenUsage.length; i++) {
        tokensToRemove -= this.tokenUsage[i].tokens;
        if (tokensToRemove <= 0) {
          // Wait until this entry expires (1 minute from its timestamp)
          return Math.max(0, (this.tokenUsage[i].timestamp + 60000) - Date.now());
        }
      }
    }
    
    if (currentRequests >= this.requestsPerMinute) {
      // Wait until the oldest request expires
      return Math.max(0, (this.requests[0].timestamp + 60000) - Date.now());
    }
    
    return 0;
  }
}

// Global token tracker instance
const tokenTracker = new TokenTracker();

// Enhanced OpenAI API call with intelligent rate limiting
async function callOpenAIWithRateLimit(messages, maxTokens = 4000, retries = 2) {
  const estimatedInputTokens = JSON.stringify(messages).length / 4; // Rough estimate
  const estimatedTotalTokens = estimatedInputTokens + maxTokens;
  
  // Check if we need to wait before making the request
  if (!tokenTracker.canMakeRequest(estimatedTotalTokens)) {
    const waitTime = tokenTracker.getWaitTime(estimatedTotalTokens);
    if (waitTime > 0) {
      logWithTimestamp(`‚è≥ Proactive rate limit wait: ${waitTime}ms to avoid hitting limits`);
      await new Promise(resolve => setTimeout(resolve, waitTime));
    }
  }

  const startTime = Date.now();
  
  for (let attempt = 0; attempt <= retries; attempt++) {
    try {
      logWithTimestamp(`üîÑ Executing OpenAI API call with ${retries - attempt} retries left`);
      
      const response = await openai.chat.completions.create({
        model: "gpt-4o-mini",
        messages: messages,
        max_tokens: maxTokens,
        temperature: 0.1,
      });

      const duration = Date.now() - startTime;
      logWithTimestamp('‚úÖ OpenAI API call successful', { duration: `${duration}ms` });
      
      // Track successful token usage
      const tokensUsed = response.usage?.total_tokens || estimatedTotalTokens;
      tokenTracker.addTokenUsage(tokensUsed);
      
      return response;
      
    } catch (error) {
      const duration = Date.now() - startTime;
      const isRateLimit = error.status === 429;
      const isLastAttempt = attempt === retries;
      
      logWithTimestamp('‚ùå OpenAI API call failed', {
        duration: `${duration}ms`,
        error: error.message,
        code: error.code,
        status: error.status,
        headers: error.headers
      });

      if (isRateLimit && !isLastAttempt) {
        // Extract wait time from headers or use exponential backoff
        const retryAfter = error.headers?.['retry-after-ms'] || error.headers?.['retry-after'] * 1000 || (Math.pow(2, attempt) * 1000);
        const waitTime = Math.min(retryAfter + 1000, 60000); // Cap at 60 seconds
        
        logWithTimestamp(`‚è≥ Rate limit hit, waiting ${waitTime}ms before retry. Retries left: ${retries - attempt - 1}`);
        await new Promise(resolve => setTimeout(resolve, waitTime));
        continue;
      }
      
      if (isLastAttempt) {
        throw error;
      }
    }
  }
}

export async function POST(request) {
  let requestId = `req_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  const startTime = Date.now();
  
  logWithTimestamp('üöÄ Starting new analysis request', { requestId });
  
  let videoPath = '';
  let requestBody = null;
  
  try {
    // Parse request body
    logWithTimestamp('üì• Parsing request body', { requestId });
    requestBody = await request.json();
    const { url, userId, estimatedCredits, requestId: frontendRequestId } = requestBody;
    
    // Use the requestId from frontend if provided, otherwise keep the generated one
    if (frontendRequestId) {
      requestId = frontendRequestId;
    }
    
    logWithTimestamp('üìã Request details', { 
      requestId,
      url,
      userAgent: request.headers.get('user-agent'),
      origin: request.headers.get('origin')
    });
    
    // Validate input
    if (!url) {
      logWithTimestamp('‚ùå Missing URL in request', { requestId });
      return NextResponse.json({ 
        error: 'URL is required',
        requestId 
      }, { status: 400 });
    }

    // Validate environment or enable demo mode
    if (!process.env.OPENAI_API_KEY) {
      logWithTimestamp('‚ö†Ô∏è Missing OpenAI API key - using demo mode', { requestId });
      
      // Return demo analysis for testing UI
      const demoAnalysis = {
        contentStructure: "Demo analysis mode - OpenAI API not configured",
        hook: "Demo Hook: This is a sample analysis",
        totalDuration: "6.0s",
        scenes: [
          {
            sceneNumber: 1,
            duration: "3.0s",
            timeRange: "0:00 - 0:03",
            title: "Demo Scene 1",
            description: "Sample scene for testing UI",
            framing: { shotTypes: ["Medium Shot"], cameraMovement: "Static", composition: "Centered" },
            lighting: { style: "Natural", mood: "Neutral", direction: "Front", quality: "Good" },
            mood: { emotional: "Neutral", atmosphere: "Calm", tone: "Professional" },
            action: { movement: "Minimal", direction: "Static", pace: "Slow" },
            dialogue: { hasText: true, textContent: "Demo text content", textStyle: "Simple" },
            audio: { music: "Background", soundDesign: "Minimal", dialogue: "Clear" },
            visualEffects: { transitions: "Cut", effects: "None", graphics: "Text overlay" },
            setting: { location: "Indoor", environment: "Office", background: "Neutral" },
            subjects: { main: "Person", secondary: "Objects", focus: "Center" },
            contextualMeaning: {
              intent: "Demonstrate the UI functionality",
              execution: "Simple demo implementation",
              impact: "Shows how the analysis would appear",
              significance: "Testing interface without API costs"
            }
          },
          {
            sceneNumber: 2,
            duration: "3.0s",
            timeRange: "0:03 - 0:06",
            title: "Demo Scene 2",
            description: "Second sample scene",
            framing: { shotTypes: ["Close-up"], cameraMovement: "Pan", composition: "Off-center" },
            lighting: { style: "Artificial", mood: "Warm", direction: "Side", quality: "Excellent" },
            mood: { emotional: "Positive", atmosphere: "Energetic", tone: "Engaging" },
            action: { movement: "Moderate", direction: "Left to right", pace: "Medium" },
            dialogue: { hasText: false, textContent: "", textStyle: "" },
            audio: { music: "Upbeat", soundDesign: "Enhanced", dialogue: "None" },
            visualEffects: { transitions: "Fade", effects: "Motion blur", graphics: "None" },
            setting: { location: "Outdoor", environment: "Street", background: "Urban" },
            subjects: { main: "Movement", secondary: "Background", focus: "Dynamic" },
            contextualMeaning: {
              intent: "Show scene progression",
              execution: "Smooth transition",
              impact: "Maintains viewer engagement",
              significance: "Demonstrates pacing"
            }
          }
        ],
        transcript: { text: "Demo transcript: This is a sample analysis without actual audio processing.", segments: [] },
        hooks: [],
        videoCategory: { category: "tutorial", confidence: 0.8, reasoning: "Demo mode active", keyIndicators: ["demo", "testing"] },
        contextualAnalysis: {
          creatorIntent: { primaryIntent: "Testing the interface", howAchieved: "Demo mode", effectivenessFactors: ["UI testing"] },
          humorMechanics: { isHumorous: false, humorType: "none", specificTechniques: [], setupAndPayoff: "", timingElements: "" },
          narrativeStructure: { setup: "Demo setup", conflict: "API unavailable", resolution: "Fallback mode", storytellingDevices: ["demo"] },
          visualTechniques: { textElements: "Demo text", visualEffects: "Demo effects", facialExpressions: "N/A", visualContrasts: "Demo contrast" },
          characters: [],
          messageDelivery: { coreMessage: "Demo mode active", deliveryMethod: "Fallback", memorabilityFactors: ["demo"], audienceAssumptions: ["testing"] },
          contextType: "demo",
          targetAudience: "developers",
          keyInsights: ["This is demo mode - configure OpenAI API key for real analysis"]
        },
        strategicOverview: `## Video Analysis Overview (Demo Mode)

**Content Type:** Demo Tutorial
**Duration:** 2 scenes analyzed  
**Primary Appeal:** Interface Testing

### Key Success Elements:
- Scene 1: Demonstrate the UI functionality
- Scene 2: Show scene progression

### Strategic Breakdown

#### Why It Works: Interface Testing
- Provides immediate visual feedback without API costs
- Allows UI testing and development iteration
- Shows the complete analysis structure

#### Success Formula: Demo Implementation
1. **Structure**: 2-scene progression for basic testing
2. **Timing**: Balanced 3-second scenes
3. **Content**: Covers all analysis categories

#### Universal Principles: Development Testing
- Immediate feedback for developers
- Complete UI coverage without external dependencies  
- Safe testing environment

#### Technical Requirements: Configuration Needed
- **Critical**: Valid OpenAI API key for real analysis
- **Optional**: Sufficient API credits for video processing
- **Recommended**: Test with demo mode first

### Replication Insights

#### Implementation Framework
1. Configure OpenAI API key in environment variables
2. Ensure sufficient API credits
3. Test with short videos first
4. Monitor API usage and costs

#### Resource Requirements
- **Demo Mode**: No API costs, instant results
- **Full Analysis**: API credits required (1 credit per 15 seconds)
- **Development**: Use demo mode for UI testing

*Note: This is demo mode. Configure your OpenAI API key for full video analysis capabilities.*`,
        videoMetadata: {
          totalFrames: 12,
          frameRate: 2,
          analysisTimestamp: new Date().toISOString()
        }
      };
      
      return NextResponse.json({
        ...demoAnalysis,
        requestId,
        processingTime: "1ms",
        isDemoMode: true
      });
    }

    logWithTimestamp('‚úÖ Environment validation passed', { requestId });

    // Start video analysis
    logWithTimestamp('üé¨ Starting video analysis pipeline', { requestId, url });

    videoPath = await downloadVideo(url);
    logWithTimestamp('‚úÖ Video download completed', { requestId, videoPath });
    
    // Initialize credits deduction value
    let creditsToDeduct = typeof estimatedCredits === 'number' ? estimatedCredits : 0;

    // If Supabase and userId provided, ensure sufficient balance before analysis
    if (userId && isSupabaseAvailable()) {
      try {
        let profile = await getUserProfile(userId);
        if (!profile) {
          // Auto-create user profile with default starting credits
          const DEFAULT_INITIAL_CREDITS = 10;
          const { data: newProfile, error: insertErr } = await supabase
            .from('user_profiles')
            .insert({ id: userId, credits_balance: DEFAULT_INITIAL_CREDITS })
            .select()
            .single();
          if (insertErr) {
            console.error('Failed to auto-create profile', insertErr);
            // Gracefully allow analysis without credits deduction
            profile = { credits_balance: 0 };
          } else {
            profile = newProfile;
          }
        }
        const balance = profile.credits_balance ?? profile.credits ?? 0;
        if (creditsToDeduct && balance < creditsToDeduct) {
          return NextResponse.json({ error: 'Insufficient credits' }, { status: 402 });
        }
      } catch (supabaseErr) {
        console.error('Supabase check failed', supabaseErr);
      }
    }
    
    const analysis = await analyzeVideo(videoPath, userId, creditsToDeduct, requestId);
    logWithTimestamp('‚úÖ Video analysis completed', { requestId });
    
    // Final cleanup
    await cleanupFile(videoPath);
    
    const totalDuration = Date.now() - startTime;
    logWithTimestamp('üéâ Request completed successfully!', { 
      requestId,
      totalDuration: `${totalDuration}ms`,
      analysisKeys: Object.keys(analysis)
    });
    
    return NextResponse.json({
      ...analysis,
      requestId,
      processingTime: `${totalDuration}ms`
    });
  } catch (error) {
    const totalDuration = Date.now() - startTime;
    logWithTimestamp('üí• Request failed with error', { 
      requestId,
      error: error.message,
      stack: error.stack,
      totalDuration: `${totalDuration}ms`,
      requestBody,
      videoPath
    });
    
    // Ensure cleanup even on error
    if (videoPath) {
      await cleanupFile(videoPath);
    }

    // Clean up frames directory if it exists
    const framesDir = path.join(process.cwd(), 'temp', 'frames');
    if (fs.existsSync(framesDir)) {
      try {
        const frameFiles = fs.readdirSync(framesDir);
        logWithTimestamp('üßπ Emergency cleanup of frames directory', { 
          frameCount: frameFiles.length,
          requestId
        });
      fs.rmSync(framesDir, { recursive: true, force: true });
        logWithTimestamp('‚úÖ Emergency cleanup completed', { requestId });
      } catch (cleanupError) {
        logWithTimestamp('‚ùå Emergency cleanup failed', { 
          error: cleanupError.message,
          requestId
        });
      }
    }
    
    return NextResponse.json({ 
      error: 'Failed to analyze video', 
      details: error.message,
      requestId,
      processingTime: `${totalDuration}ms`
    }, { status: 500 });
  }
}
